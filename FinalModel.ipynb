{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ca9078-71cf-41d4-aec7-2b775ab3188f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f3a5a4e6710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aicspylibczi import CziFile\n",
    "import czifile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "import ffmpeg\n",
    "import time\n",
    "import pandas as pd\n",
    "# from cellpose import io, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "cudnn.benchmark = True\n",
    "from VideoLoaders import *\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c4a425c-002e-4cdc-bafa-8f7f024123fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataMIP:\n",
    "    def __init__(self, files):\n",
    "        self.data = {\n",
    "        }\n",
    "        \n",
    "        for category, num in files:\n",
    "            print(f\"Loading in MIP {num}\")\n",
    "            print(category)\n",
    "            # assert category == 'mip', \"Can't load non Mip file\"\n",
    "            file = {}\n",
    "            file['video'] = get_file(category, num)\n",
    "            \n",
    "            frames, shp = file['video'].read_image(C=0)\n",
    "            frames = scale_img(frames.squeeze())\n",
    "            file['frames'] = frames\n",
    "            print(f\"frames {num}: {frames.shape}\")\n",
    "            file['masks'] = binarize_video(frames)           \n",
    "    \n",
    "            self.data[num] = file    \n",
    "    def extract_all_traces(self, file_num, sequence_length, hist_length=2):\n",
    "        # hist length is how many frames of history\n",
    "        frames, masks = self.data[file_num]['frames'], self.data[file_num]['masks']\n",
    "        N = len(frames)\n",
    "        s = 0\n",
    "        all_traces = []\n",
    "        all_videos = []\n",
    "        for i in range(N // sequence_length):\n",
    "            print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "            data, videos = extract_traces(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "            s += sequence_length\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos\n",
    "        \n",
    "        if(N % sequence_length > 0):\n",
    "            data, videos = extract_traces(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos        \n",
    "        self.data[file_num]['traces'] = all_traces\n",
    "        self.data[file_num]['trace_videos'] = all_videos\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ed79b05-c94d-476e-b64d-a577539b67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import centroid\n",
    "import skimage.measure as skm\n",
    "\n",
    "max_padding =  300\n",
    "\n",
    "box_shape = (180, 180) #TO DO: find the biggest box and set it to this\n",
    "X = 10\n",
    "\n",
    "class CellBoxMaskPatch(torch.utils.data.Dataset):\n",
    "    #input will be a Directory name, function is TO DO\n",
    "    def __init__(\n",
    "        self,\n",
    "        files, \n",
    "        X=X):\n",
    "        \n",
    "        self.mips_extractor = VideoDataMIP(files)\n",
    "        # self.proc_extractor = VideoDataProcessed(files)\n",
    "\n",
    "        for i in files:\n",
    "            self.mips_extractor.extract_all_traces(i[1], X)\n",
    "        \n",
    "        \n",
    "        self.cell_dict = []\n",
    "\n",
    "        for key in self.mips_extractor.data:\n",
    "            entry = self.mips_extractor.data[key][\"traces\"]\n",
    "            for cell in entry:\n",
    "                patches = [np.array(p) for p in cell[\"patches\"]]\n",
    "                boxes = [np.array(b) for b in cell['boxes']]\n",
    "                masks = [np.array(m) for m in cell['masks']]\n",
    "                \n",
    "                self.cell_dict.append((boxes, masks, patches)) #cell dict is a list of 3 types by sequence\n",
    "\n",
    "        self.num_cells = len(self.cell_dict) #this is a list of how many sequences we have\n",
    "              \n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "        \n",
    "\n",
    "    def get_centroids(self, boxes, masks):\n",
    "        N = len(masks)\n",
    "        res = []\n",
    "        centroids = [skm.centroid(binary.astype(np.uint8)) for binary in masks]\n",
    "        for i in range(N):\n",
    "            c = centroids[i]\n",
    "            ymin, xmin = boxes[i][:2]\n",
    "            res.append([xmin+c[0], ymin+c[1]])\n",
    "        return(np.array(res) - res[0]) \n",
    "   \n",
    "    def pad_arrays(self, array, pad_amt=max_padding):\n",
    "    \n",
    "        pad_width = ((0, pad_amt - array.shape[0]), (0, pad_amt - array.shape[1]))\n",
    "\n",
    "        padded_array = np.pad(array, pad_width, mode='constant')\n",
    "        return padded_array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_sequences = self.cell_dict[idx]  #this is the first sequence of 10 cells\n",
    "        boxes = cell_sequences[0]\n",
    "        masks = cell_sequences[1]\n",
    "        patches = cell_sequences[2]\n",
    "\n",
    "\n",
    "        for cell_mask_num in np.arange(len(masks)): #should be sequence length (10) masks\n",
    "                \n",
    "                cell_time = np.array(masks[cell_mask_num], dtype=np.int32)\n",
    "                cell_time = np.where(cell_time >= 0, cell_time, 1)\n",
    "                cell_time = self.pad_arrays(cell_time)\n",
    "                masks[cell_mask_num] = cell_time\n",
    "                cell_time_patch = np.array(patches[cell_mask_num], dtype=np.int32)\n",
    "\n",
    "                cell_time_patch = self.pad_arrays(cell_time_patch)\n",
    "\n",
    "                patches[cell_mask_num] = cell_time_patch\n",
    "\n",
    "\n",
    "        centroids = self.get_centroids(boxes, masks)\n",
    "    \n",
    "\n",
    "        return centroids, masks, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c99cccaa-0e29-4973-9cf4-fca24e280e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "mip_video_files = [\n",
    "    ('mip', 3),\n",
    "    ('mip', 6),\n",
    "    ('mip', 9)\n",
    "]\n",
    "\n",
    "# dataset = CellBoxMaskPatch(mip_video_files, X) # file, S, T\n",
    "\n",
    "# train, val, test = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "# input_datasets = {}\n",
    "# input_datasets[\"train\"] = train\n",
    "# input_datasets[\"val\"] = val\n",
    "# input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6123bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import *\n",
    "# import mahotas #: Module(\"mahotas\")\n",
    "\n",
    "\n",
    "# def extract_traces_sparse(frames, masks, hist=2):\n",
    "#     bboxes, num_cells, areas = bounding_boxes(masks[0])\n",
    "#     vid_data = []\n",
    "#     for i in range(num_cells):\n",
    "#         #print(\"Extracting cell \", i)\n",
    "#         data = track_cells(i, frames, masks, padding=0, history_length=hist, verbose=False)\n",
    "#         vid_data.append(data)\n",
    "#     return(vid_data)\n",
    "\n",
    "# def shape_features(binary, feature_length=20, num_samples=180):\n",
    "#     def radial_distance(binary, theta):\n",
    "#         height, width = binary.shape\n",
    "#         center = [width // 2, height // 2]\n",
    "#         def test_r(r):\n",
    "#             x_test, y_test = center[0] + r*np.cos(theta), center[1] + r*np.sin(theta)\n",
    "#             if(x_test >= width or y_test > height or x_test < 0 or y_test < 0):\n",
    "#                 return(False)\n",
    "#             return(binary[int(y_test), int(x_test)])\n",
    "#         # calculate distance to the nearest pixel\n",
    "#         r = max(height, width)\n",
    "#         while(not test_r(r)): # start from edge come inside until hit cell\n",
    "#             r -= 1\n",
    "#         return(r)\n",
    "\n",
    "#     test_angles = np.linspace(0, 2*np.pi, num_samples)\n",
    "#     distances = np.array([radial_distance(binary, angle) for angle in test_angles])\n",
    "#     fft_coefficients = np.fft.rfft(distances)\n",
    "\n",
    "#     features = np.abs(fft_coefficients[:feature_length])\n",
    "#     features = features / np.sum(features)\n",
    "#     return(features, (distances, fft_coefficients))\n",
    "\n",
    "# def featurize(cell_data, index):\n",
    "#     image, binary = cell_data['patches'][index], cell_data['masks'][index].astype(np.uint8)\n",
    "#     zernike = mahotas.features.zernike_moments(binary, max(binary.shape)/2, degree=8)\n",
    "#     #zernike = zernike / zernike.sum()\n",
    "#     haralick = mahotas.features.haralick(image.astype(np.uint16)).mean(axis=0)\n",
    "#     #haralick = haralick / haralick.sum()\n",
    "#     shape, info = shape_features(binary, 20)\n",
    "#     #print(f\"Zernike: {zernike.shape}, Haralick: {haralick.shape}, Radial Shape: {shape.shape}\")\n",
    "#     return(np.concatenate([zernike, haralick, shape]))\n",
    "\n",
    "# class VideoDataProcessed:\n",
    "#     def __init__(self, files, sequence_length=5, channel=0):\n",
    "#         self.data = {}\n",
    "#         self.all_traces = []\n",
    "#         self.seq_length = sequence_length\n",
    "#         self.channel = channel\n",
    "#         self.videos = {}\n",
    "#         for category, num in files:\n",
    "#             print(f\"Loading in processed {num}\")\n",
    "#             assert category == 'processed', \"Can't load non processed file\"\n",
    "#             video = get_file(category, num)\n",
    "#             self.videos[num] = video\n",
    "#         self.num_vids = len(self.data)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_vids\n",
    "\n",
    "#     def extract_planes(self, num, zplanes, hist_length):\n",
    "#         for z in zplanes:\n",
    "#             self.extract_slice_traces(num, z, hist_length)\n",
    "    \n",
    "#     def extract_slice_traces(self, num, zPlane, hist_length=2):\n",
    "#         assert num in self.videos.keys(), f\"Video {num} not found\"\n",
    "        \n",
    "#         video = self.videos[num]\n",
    "#         frames, shp = video.read_image(C=self.channel, S=0, Z=zPlane)\n",
    "#         frames = scale_img(frames.squeeze())\n",
    "#         print(f\"vid {num} zplane {zPlane} with frames: {frames.shape}\")\n",
    "#         masks = binarize_video(frames)\n",
    "#         N = len(frames)\n",
    "#         s = 0\n",
    "#         for i in range(N // self.seq_length):\n",
    "#             print(f\"Extracting traces from {s}:{s+self.seq_length}\")\n",
    "#             data = extract_traces_sparse(frames[s:s+self.seq_length], masks[s:s+self.seq_length], hist=hist_length)\n",
    "#             s += self.seq_length\n",
    "#             self.all_traces = self.all_traces + data\n",
    "        \n",
    "#         if(N % self.seq_length > 0):\n",
    "#             data = extract_traces_sparse(frames[-1*self.seq_length:], masks[-1*self.seq_length:], hist=hist_length)\n",
    "#             self.all_traces = self.all_traces + data\n",
    "\n",
    "\n",
    "# class SparseMIPVideo:\n",
    "#     def __init__(self, files, sequence_length, hist_length=2):\n",
    "#         self.data = {}\n",
    "#         self.all_traces = []\n",
    "#         self.N = sequence_length\n",
    "#         for category, num in files:\n",
    "#             print(f\"Loading in MIP {num}\")\n",
    "#             assert category == 'mip', \"Can't load non Mip file\"\n",
    "#             video = get_file(category, num)\n",
    "#             frames, shp = video.read_image(C=0)\n",
    "#             frames = scale_img(frames.squeeze())\n",
    "#             print(f\"frames {num}: {frames.shape}\")\n",
    "#             masks = binarize_video(frames)\n",
    "\n",
    "#             print(f\"Finished loading frames and masks for MIP {num}\")\n",
    "\n",
    "#             N = len(frames)\n",
    "#             s = 0\n",
    "        \n",
    "#             for i in range(N // sequence_length):\n",
    "#                 print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "#                 data = extract_traces_sparse(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "#                 s += sequence_length\n",
    "#                 self.all_traces = self.all_traces + data\n",
    "            \n",
    "#             if(N % sequence_length > 0):\n",
    "#                 data = extract_traces_sparse(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "#                 self.all_traces = self.all_traces + data\n",
    "\n",
    "#     def featurize_traces(self):\n",
    "#         self.featurized_frames = []\n",
    "#         for i, trace in enumerate(self.all_traces):\n",
    "#             if(i % 100 == 0):\n",
    "#                 print(i)\n",
    "#             trajectory_features = np.array([featurize(trace, index) for index in range(5)])\n",
    "#             self.featurized_frames.append(trajectory_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e324da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in MIP 3\n",
      "mip\n",
      "Loading dicty_factin_pip3-03_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 90)}]\n",
      "frames 3: (90, 2048, 474)\n",
      "Loading in MIP 6\n",
      "mip\n",
      "Loading dicty_factin_pip3-06_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 241)}]\n",
      "frames 6: (241, 2048, 474)\n",
      "Loading in MIP 9\n",
      "mip\n",
      "Loading dicty_factin_pip3-09_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 241)}]\n",
      "frames 9: (241, 2048, 474)\n",
      "Extracting traces from 0:10\n",
      "Extracting traces from 10:20\n",
      "Extracting traces from 20:30\n",
      "Extracting traces from 30:40\n",
      "Extracting traces from 40:50\n",
      "Extracting traces from 50:60\n",
      "Extracting traces from 60:70\n",
      "Extracting traces from 70:80\n",
      "Extracting traces from 80:90\n",
      "Extracting traces from 0:10\n",
      "Extracting traces from 10:20\n",
      "Extracting traces from 20:30\n",
      "Extracting traces from 30:40\n",
      "Extracting traces from 40:50\n",
      "Extracting traces from 50:60\n",
      "Extracting traces from 60:70\n",
      "Extracting traces from 70:80\n",
      "Extracting traces from 80:90\n",
      "Extracting traces from 90:100\n",
      "Extracting traces from 100:110\n",
      "Extracting traces from 110:120\n",
      "Extracting traces from 120:130\n",
      "Extracting traces from 130:140\n",
      "Extracting traces from 140:150\n",
      "Extracting traces from 150:160\n",
      "Extracting traces from 160:170\n",
      "Extracting traces from 170:180\n",
      "Extracting traces from 180:190\n",
      "Extracting traces from 190:200\n",
      "Extracting traces from 200:210\n",
      "Extracting traces from 210:220\n",
      "Extracting traces from 220:230\n",
      "Extracting traces from 230:240\n",
      "Extracting traces from 0:10\n",
      "Extracting traces from 10:20\n",
      "Extracting traces from 20:30\n",
      "Extracting traces from 30:40\n",
      "Extracting traces from 40:50\n",
      "Extracting traces from 50:60\n",
      "Extracting traces from 60:70\n",
      "Extracting traces from 70:80\n",
      "Extracting traces from 80:90\n",
      "Extracting traces from 90:100\n",
      "Extracting traces from 100:110\n",
      "Extracting traces from 110:120\n",
      "Extracting traces from 120:130\n",
      "Extracting traces from 130:140\n",
      "Extracting traces from 140:150\n",
      "Extracting traces from 150:160\n",
      "Extracting traces from 160:170\n",
      "Extracting traces from 170:180\n",
      "Extracting traces from 180:190\n",
      "Extracting traces from 190:200\n",
      "Extracting traces from 200:210\n",
      "Extracting traces from 210:220\n",
      "Extracting traces from 220:230\n",
      "Extracting traces from 230:240\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "mip_video_files = [\n",
    "    ('mip', 3),\n",
    "    ('mip', 6),\n",
    "    ('mip', 9)\n",
    "]\n",
    "\n",
    "dataset = CellBoxMaskPatch(mip_video_files, X) # file, S, T\n",
    "\n",
    "train, eval, test = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "input_datasets = {}\n",
    "input_datasets[\"train\"] = train\n",
    "input_datasets[\"eval\"] = eval\n",
    "input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f44453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "# import VideoLoaders\n",
    "\n",
    "# X = 10\n",
    "# processed_video_files = [\n",
    "#     ('processed', 3),\n",
    "# ]\n",
    "# processed_dataset = VideoLoaders.VideoDataProcessed(processed_video_files)\n",
    "# processed_dataset.extract_slice_traces(3, 50)\n",
    "\n",
    "# # train, val, test = random_split(processed_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "# # input_datasets = {}\n",
    "# # input_datasets[\"train\"] = train\n",
    "# # input_datasets[\"val\"] = val\n",
    "# # input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "314d04ff-2a4d-4226-9838-026a10d37ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, mode_box, mode_mask, mode_patch):\n",
    "    current_centroids = [b[0] for b in batch]\n",
    "    current_masks = [b[1] for b in batch]\n",
    "    current_patches = [b[2] for b in batch]\n",
    "\n",
    "    current_centroids = torch.tensor(np.stack(current_centroids), dtype=torch.float32)\n",
    "    current_masks = torch.tensor(np.stack(current_masks), dtype=torch.long)\n",
    "    current_patches = torch.tensor(np.stack(current_patches), dtype=torch.long)\n",
    "\n",
    "    current_patches = current_patches.reshape([len(batch), 10, max_padding*max_padding])\n",
    "    current_masks = current_masks.reshape([len(batch), 10, max_padding*max_padding])\n",
    "\n",
    "    selected_tensors = []\n",
    "    if mode_box:\n",
    "        selected_tensors.append(current_centroids)\n",
    "    if mode_mask:\n",
    "        selected_tensors.append(current_masks)\n",
    "    if mode_patch:\n",
    "        selected_tensors.append(current_patches)\n",
    "\n",
    "    combined_tensor = torch.cat(selected_tensors, dim=-1)\n",
    "\n",
    "    # Cast the combined tensor to torch.float32\n",
    "    combined_tensor = combined_tensor.to(torch.float32)\n",
    "\n",
    "    return combined_tensor, current_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3f83b05-0e73-4f12-8ca1-a207a201b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box is actually a box surrounding the cell\n",
    "# mask is the values of the cell\n",
    "# patch is fluorescence\n",
    "mode_box = False\n",
    "mode_mask = False\n",
    "mode_patch = True\n",
    "\n",
    "input_size = 0\n",
    "if mode_box:\n",
    "    input_size+=2\n",
    "if mode_mask:\n",
    "    input_size+=max_padding*max_padding\n",
    "if mode_patch:\n",
    "    input_size+=max_padding*max_padding\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['test'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['eval'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['eval'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "45f225f6-9e7f-4b15-bd8b-34d63d3b8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([3, 10, 90000]) Centroids torch.Size([3, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders['eval']:\n",
    "    if (batch[0].shape != torch.zeros([4, 10, 90000]).shape):\n",
    "        print(\"Input:\", batch[0].shape, \"Centroids\", batch[1].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ab035",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "568655b8-8ed2-4097-829f-0423b5afdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d191cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9dbae0a-4601-4383-a79d-596d32dab831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True) #stacking 2 LSTMs\n",
    "        # hidden out output\n",
    "        #  2 bc x y centroid\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "    def forward(self, input):\n",
    "        #h_0: tensor of shape (D∗num_layers,N,Hout​) containing the initial hidden\n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).to(input.device)\n",
    "        c0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).to(input.device)\n",
    "\n",
    "       \n",
    "#         out, _ = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "        out, _ = self.lstm(input, (h0, c0))\n",
    "\n",
    "        out = self.fc(out)\n",
    "        final = out[:,-1,:]\n",
    "        out = torch.sigmoid(final) * max_padding\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(ImprovedLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2) \n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional LSTM\n",
    "        c0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional LSTM\n",
    "\n",
    "        out, _ = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14627efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(MultiLayerLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  \n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  \n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  \n",
    "        c0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  \n",
    "\n",
    "        # out, _ = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "        out, _ = self.lstm(input, (h0, c0))\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "\n",
    "        out = out.to(torch.float32)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ab2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(MultiLayerGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  \n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  \n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  \n",
    "\n",
    "        out, _ = self.gru(input, h0.detach())\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "\n",
    "        out = out.to(torch.float32)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerComplexGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(MultiLayerComplexGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru1 = nn.GRU(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 2)  \n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)  \n",
    "        self.fc3 = nn.Linear(hidden_dim, 2) \n",
    "\n",
    "    def forward(self, input):\n",
    "        # Init\n",
    "        h0_1 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional GRU\n",
    "        h0_2 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional GRU\n",
    "\n",
    "        out, _ = self.gru1(input, h0_1.detach())\n",
    "        \n",
    "        out, _ = self.gru2(out, h0_2.detach())\n",
    "\n",
    "        # Predict to mad padding\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "\n",
    "        out = out.to(torch.float32)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2a95b-40a9-421d-a3f2-f8218a4a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrivialLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=2, hidden_size=1000, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(1000, 2)  # To ensure the output size matches the input size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        output, _ = self.lstm(x)\n",
    "        # Linear layer to match the output size to input size\n",
    "        output = self.linear(output)\n",
    "        return output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb34bf3-5281-4f7a-8f26-020550b01dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        #Flatten the input from (4, 9, 2) to (4, 18)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(810000, 500) \n",
    "        self.fc2 = nn.Linear(500,1000)\n",
    "        self.fc3 = nn.Linear(1000,500)\n",
    "        self.fc4 = nn.Linear(500, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) \n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8438fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(32 * 5 * 5, 500) \n",
    "#         self.fc2 = nn.Linear(500,1000)\n",
    "#         self.fc3 = nn.Linear(1000,500)\n",
    "#         self.fc4 = nn.Linear(500, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = torch.relu(self.fc1(x)) \n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         x = self.fc4(x)  \n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "928c7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Adjust the input size of fc1 based on the output size after convolutions and pooling\n",
    "        self.fc1 = nn.Linear(1440000, 500) \n",
    "        self.fc2 = nn.Linear(500,1000)\n",
    "        self.fc3 = nn.Linear(1000,500)\n",
    "        self.fc4 = nn.Linear(500, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"prereshaping\", x.shape)\n",
    "        x = x = torch.reshape(x, (x.shape[0], 1, x.shape[1], x.shape[2]))\n",
    "        # print(\"zero\", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(\"after first\", x.shape)\n",
    "        x = torch.relu(x)\n",
    "        # print(\"after secon\", x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(\"after third\", x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(\"after fourth\", x.shape)\n",
    "        x = torch.relu(x)\n",
    "        # print(\"after fifth\", x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(\"after six\", x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print(\"after seven\", x.shape)\n",
    "        x = (self.fc1(x)) \n",
    "        # print(\"after eight\", x.shape)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # print(\"after nine\", x.shape)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        # print(\"after ten\", x.shape)\n",
    "        x = self.fc4(x)  \n",
    "        # print(\"after last fc4\", x.shape)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3aa1d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleCNN(nn.Module):\n",
    "\n",
    "# # inputs torch.Size([4, 10, 90000])\n",
    "# # outputs torch.Size([4, 10, 2])\n",
    "\n",
    "# # first torch.Size([4, 9, 90000])\n",
    "# # second torch.Size([4, 16, 90000])\n",
    "# # third torch.Size([4, 4, 90000])\n",
    "# # four torch.Size([16, 90000])\n",
    "# # five torch.Size([16, 128])\n",
    "# # six torch.Size([16, 2])\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=9, out_channels=16, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=16, out_channels=4, kernel_size=3, padding=1)  \n",
    "#         self.conv3 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=3, padding=1)  \n",
    "#         self.fc1 = nn.Linear(4 * 90000 // 4, 128)  \n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(\"first\", x.shape)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         print(\"second\", x.shape)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         print(\"third\", x.shape)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         print(\"four\", x.shape)\n",
    "#         x = x.view(-1, 16 * 90000 // 4)  \n",
    "#         print(\"five\", x.shape)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         # print(\"five\", x.shape)\n",
    "#         x = self.fc2(x)\n",
    "#         # print(\"six\", x.shape)\n",
    "#         return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0bbb82b3-20cd-44eb-9f9f-43a55697cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 2000\n",
    "num_layers = 2\n",
    "epochs = 1000\n",
    "sequence_length = 10 #how many frames we process per input\n",
    "\n",
    "# model = LSTM(input_size, hidden_size, num_layers)\n",
    "model = SimpleCNN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# dummy_input_data = torch.randn(batch_size, 10, input_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in dataloaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, outputs = batch[0], batch[1]\n",
    "        # print(\"inputs\", inputs.shape)\n",
    "        # print(\"outputs\", outputs.shape)\n",
    "        # shape is (batch_size * frames * input)\n",
    "        # print(inputs[:, sequence_length-1:sequence_length, :].shape)\n",
    "#         print(outputs[:,-1,:].shape)\n",
    "        inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "        pred = model(inputs[:, :sequence_length-1, :])\n",
    "        # print(\"final prediction\", pred.shape, \"should be\", outputs[:,-1,:].shape)\n",
    "        # print(inputs[:, sequence_length-1:sequence_length, :].shape)\n",
    "        # print(pred.shape, outputs[:,-1,:].shape)\n",
    "        # print(f\"pred: {pred}\")\n",
    "        # print(f\"outputs: {outputs.data[:,-1,:]}\")\n",
    "        # total_correct += torch.sum(torch.eq(pred, outputs[:,-1,:]))\n",
    "        loss = criterion(pred, outputs[:,-1,:])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # print(f\"training loss: {total_loss / len(dataloaders['train'])}, training accuracy: {total_correct / len(dataloaders['eval'])}\")\n",
    "    print(f\"training loss: {total_loss / len(dataloaders['train'])}\")\n",
    "    return model\n",
    "\n",
    "def eval():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloaders['eval']:\n",
    "            inputs, outputs = batch[0], batch[1]\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            pred = model(inputs[:, :sequence_length-1, :])\n",
    "            # print(pred.shape)\n",
    "            # print(outputs.shape)\n",
    "#             print(pred,outputs[:,-1,:])\n",
    "            \n",
    "            loss = criterion(pred, outputs[:,-1,:])\n",
    "            total_loss += loss.item()\n",
    "        # X = np.array(outputs[0, :, 0])\n",
    "        # Y = np.array(outputs[0, :, 1])\n",
    "#         plt.plot(X[:9], Y[:9], marker='o', linestyle='-')\n",
    "#         plt.scatter(X[-1], Y[-1], color='orange', label='ground truth')\n",
    "#         plt.scatter(pred[0, -1, 0],pred[0, -1, 1], color='red', label='pred')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    print(f\"validation loss: {total_loss / len(dataloaders['eval'])}\")\n",
    "    return total_loss / len(dataloaders['eval'])\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        train()\n",
    "        curr_acc = eval()\n",
    "        if curr_acc > best_acc:\n",
    "            best_acc = curr_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "32d560a2-0af0-4374-bc65-739e0d8713e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "training loss: 3031.4500021389554\n",
      "validation loss: 3140.9256660461424\n",
      "Epoch: 1\n",
      "training loss: 3033.943455532619\n",
      "validation loss: 3130.4540813446047\n",
      "Epoch: 2\n",
      "training loss: 3032.5761447361538\n",
      "validation loss: 3146.8369857788084\n",
      "Epoch: 3\n",
      "training loss: 3028.650069972447\n",
      "validation loss: 3136.6823382854463\n",
      "Epoch: 4\n",
      "training loss: 3061.675450325012\n",
      "validation loss: 3147.965467071533\n",
      "Epoch: 5\n",
      "training loss: 3029.512024988447\n",
      "validation loss: 3131.8399971008303\n",
      "Epoch: 6\n",
      "training loss: 3020.81562298366\n",
      "validation loss: 3295.6486558914185\n",
      "Epoch: 7\n",
      "training loss: 3018.6280374254497\n",
      "validation loss: 3120.8861066818235\n",
      "Epoch: 8\n",
      "training loss: 3008.770206941877\n",
      "validation loss: 3249.3501098632814\n",
      "Epoch: 9\n",
      "training loss: 3004.3956031799316\n",
      "validation loss: 3159.351169395447\n",
      "Epoch: 10\n",
      "training loss: 2990.8462092672075\n",
      "validation loss: 3137.0201399803163\n",
      "Epoch: 11\n",
      "training loss: 2970.8865195138114\n",
      "validation loss: 3129.067939758301\n",
      "Epoch: 12\n",
      "training loss: 2969.9689853668215\n",
      "validation loss: 3135.0791021347045\n",
      "Epoch: 13\n",
      "training loss: 2960.8241222381594\n",
      "validation loss: 3158.3841079711915\n",
      "Epoch: 14\n",
      "training loss: 2932.709579576765\n",
      "validation loss: 3158.40006942749\n",
      "Epoch: 15\n",
      "training loss: 2920.634307370867\n",
      "validation loss: 3190.0316329956054\n",
      "Epoch: 16\n",
      "training loss: 2904.8114109039307\n",
      "validation loss: 3179.222678375244\n",
      "Epoch: 17\n",
      "training loss: 2883.1741676330566\n",
      "validation loss: 3128.532374763489\n",
      "Epoch: 18\n",
      "training loss: 2863.857828589848\n",
      "validation loss: 3266.679215621948\n",
      "Epoch: 19\n",
      "training loss: 2842.3395932878766\n",
      "validation loss: 3134.0709986686707\n",
      "Epoch: 20\n",
      "training loss: 2844.0832757132393\n",
      "validation loss: 3191.055924987793\n",
      "Epoch: 21\n",
      "training loss: 2900.6723888669694\n",
      "validation loss: 3159.2302265167236\n",
      "Epoch: 22\n",
      "training loss: 2749.8364673069545\n",
      "validation loss: 3349.3916984558105\n",
      "Epoch: 23\n",
      "training loss: 2747.8404311043873\n",
      "validation loss: 3178.7921812057493\n",
      "Epoch: 24\n",
      "training loss: 2734.0081622532434\n",
      "validation loss: 3189.672005081177\n",
      "Epoch: 25\n",
      "training loss: 2697.557383782523\n",
      "validation loss: 3250.485662460327\n",
      "Epoch: 26\n",
      "training loss: 2696.255375099182\n",
      "validation loss: 3269.312359237671\n",
      "Epoch: 27\n",
      "training loss: 2656.2087429046633\n",
      "validation loss: 3215.6714210510254\n",
      "Epoch: 28\n",
      "training loss: 2702.4160555975777\n",
      "validation loss: 3258.472546005249\n",
      "Epoch: 29\n",
      "training loss: 2610.948508099147\n",
      "validation loss: 3251.587619781494\n",
      "Epoch: 30\n",
      "training loss: 2568.7000764574323\n",
      "validation loss: 3285.0546144485475\n",
      "Epoch: 31\n",
      "training loss: 2920.2787382398333\n",
      "validation loss: 3237.0216762542723\n",
      "Epoch: 32\n",
      "training loss: 2562.8928721564157\n",
      "validation loss: 3396.0307525634767\n",
      "Epoch: 33\n",
      "training loss: 2528.5024379457745\n",
      "validation loss: 3444.493396759033\n",
      "Epoch: 34\n",
      "training loss: 2529.226688330514\n",
      "validation loss: 3357.193505859375\n",
      "Epoch: 35\n",
      "training loss: 2498.726667376927\n",
      "validation loss: 3317.476958656311\n",
      "Epoch: 36\n",
      "training loss: 2487.5352569988795\n",
      "validation loss: 3297.514465713501\n",
      "Epoch: 37\n",
      "training loss: 2533.3681446892874\n",
      "validation loss: 3297.0947379112245\n",
      "Epoch: 38\n",
      "training loss: 2440.1399428503855\n",
      "validation loss: 3261.2347873687745\n",
      "Epoch: 39\n",
      "training loss: 2419.299709265573\n",
      "validation loss: 3306.4833332061767\n",
      "Epoch: 40\n",
      "training loss: 2398.7883908680506\n",
      "validation loss: 3301.7436100006103\n",
      "Epoch: 41\n",
      "training loss: 2359.4825675419397\n",
      "validation loss: 3309.070820045471\n",
      "Epoch: 42\n",
      "training loss: 2346.0002972194125\n",
      "validation loss: 3362.646553039551\n",
      "Epoch: 43\n",
      "training loss: 2328.294489846911\n",
      "validation loss: 3388.599515533447\n",
      "Epoch: 44\n",
      "training loss: 2341.9197781426565\n",
      "validation loss: 3312.8060539245607\n",
      "Epoch: 45\n",
      "training loss: 2301.622816167559\n",
      "validation loss: 3311.8363354682924\n",
      "Epoch: 46\n",
      "training loss: 2289.0648612703594\n",
      "validation loss: 3439.665804386139\n",
      "Epoch: 47\n",
      "training loss: 2238.8179420471192\n",
      "validation loss: 3324.5611295700073\n",
      "Epoch: 48\n",
      "training loss: 2260.937863813128\n",
      "validation loss: 3334.8938541412354\n",
      "Epoch: 49\n",
      "training loss: 2215.0098638807026\n",
      "validation loss: 3498.756704711914\n",
      "Epoch: 50\n",
      "training loss: 2217.2267966406685\n",
      "validation loss: 3442.3426414489745\n",
      "Epoch: 51\n",
      "training loss: 2191.382393973214\n",
      "validation loss: 3369.2409339904784\n",
      "Epoch: 52\n",
      "training loss: 2148.8714129856653\n",
      "validation loss: 3387.2458691120146\n",
      "Epoch: 53\n",
      "training loss: 2120.710798699515\n",
      "validation loss: 3378.5259552001953\n",
      "Epoch: 54\n",
      "training loss: 2102.1242381504603\n",
      "validation loss: 3406.669578552246\n",
      "Epoch: 55\n",
      "training loss: 2107.767727661133\n",
      "validation loss: 3441.4383193969725\n",
      "Epoch: 56\n",
      "training loss: 2067.268504905701\n",
      "validation loss: 3630.854178237915\n",
      "Epoch: 57\n",
      "training loss: 2063.690178135463\n",
      "validation loss: 3410.098442173004\n",
      "Epoch: 58\n",
      "training loss: 2018.8042068208968\n",
      "validation loss: 3461.321177482605\n",
      "Epoch: 59\n",
      "training loss: 2003.6827889033727\n",
      "validation loss: 3436.326586532593\n",
      "Epoch: 60\n",
      "training loss: 1987.4017055783952\n",
      "validation loss: 3550.3626663208006\n",
      "Epoch: 61\n",
      "training loss: 1971.339686693464\n",
      "validation loss: 3440.724942779541\n",
      "Epoch: 62\n",
      "training loss: 1975.7874435629165\n",
      "validation loss: 3466.9228427886965\n",
      "Epoch: 63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model()\n",
      "Cell \u001b[0;32mIn[190], line 75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m---> 75\u001b[0m     train()\n\u001b[1;32m     76\u001b[0m     curr_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_acc \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Cell \u001b[0;32mIn[190], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(f\"training loss: {total_loss / len(dataloaders['train'])}, training accuracy: {total_correct / len(dataloaders['eval'])}\")\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146ac4d-71f3-4f35-8000-175fa16b7fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752460f6-f7a6-4b21-978d-d494889ad2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
