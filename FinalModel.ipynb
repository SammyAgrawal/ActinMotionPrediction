{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ca9078-71cf-41d4-aec7-2b775ab3188f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f3a5a4e6710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aicspylibczi import CziFile\n",
    "import czifile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "import ffmpeg\n",
    "import time\n",
    "import pandas as pd\n",
    "# from cellpose import io, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "cudnn.benchmark = True\n",
    "from VideoLoaders import *\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c4a425c-002e-4cdc-bafa-8f7f024123fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataMIP:\n",
    "    def __init__(self, files):\n",
    "        self.data = {\n",
    "        }\n",
    "        \n",
    "        for category, num in files:\n",
    "            print(f\"Loading in MIP {num}\")\n",
    "            print(category)\n",
    "            # assert category == 'mip', \"Can't load non Mip file\"\n",
    "            file = {}\n",
    "            file['video'] = get_file(category, num)\n",
    "            \n",
    "            frames, shp = file['video'].read_image(C=0)\n",
    "            frames = scale_img(frames.squeeze())\n",
    "            file['frames'] = frames\n",
    "            print(f\"frames {num}: {frames.shape}\")\n",
    "            file['masks'] = binarize_video(frames)           \n",
    "    \n",
    "            self.data[num] = file    \n",
    "    def extract_all_traces(self, file_num, sequence_length, hist_length=2):\n",
    "        # hist length is how many frames of history\n",
    "        frames, masks = self.data[file_num]['frames'], self.data[file_num]['masks']\n",
    "        N = len(frames)\n",
    "        s = 0\n",
    "        all_traces = []\n",
    "        all_videos = []\n",
    "        for i in range(N // sequence_length):\n",
    "            print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "            data, videos = extract_traces(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "            s += sequence_length\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos\n",
    "        \n",
    "        if(N % sequence_length > 0):\n",
    "            data, videos = extract_traces(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos        \n",
    "        self.data[file_num]['traces'] = all_traces\n",
    "        self.data[file_num]['trace_videos'] = all_videos\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ed79b05-c94d-476e-b64d-a577539b67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import centroid\n",
    "import skimage.measure as skm\n",
    "\n",
    "max_padding =  300\n",
    "\n",
    "box_shape = (180, 180) #TO DO: find the biggest box and set it to this\n",
    "X = 10\n",
    "\n",
    "class CellBoxMaskPatch(torch.utils.data.Dataset):\n",
    "    #input will be a Directory name, function is TO DO\n",
    "    def __init__(\n",
    "        self,\n",
    "        files, \n",
    "        X=X):\n",
    "        \n",
    "        self.mips_extractor = VideoDataMIP(files)\n",
    "        # self.proc_extractor = VideoDataProcessed(files)\n",
    "\n",
    "        for i in files:\n",
    "            self.mips_extractor.extract_all_traces(i[1], X)\n",
    "        \n",
    "        \n",
    "        self.cell_dict = []\n",
    "\n",
    "        for key in self.mips_extractor.data:\n",
    "            entry = self.mips_extractor.data[key][\"traces\"]\n",
    "            for cell in entry:\n",
    "                patches = [np.array(p) for p in cell[\"patches\"]]\n",
    "                boxes = [np.array(b) for b in cell['boxes']]\n",
    "                masks = [np.array(m) for m in cell['masks']]\n",
    "                \n",
    "                self.cell_dict.append((boxes, masks, patches)) #cell dict is a list of 3 types by sequence\n",
    "\n",
    "        self.num_cells = len(self.cell_dict) #this is a list of how many sequences we have\n",
    "              \n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "        \n",
    "\n",
    "    def get_centroids(self, boxes, masks):\n",
    "        N = len(masks)\n",
    "        res = []\n",
    "        centroids = [skm.centroid(binary.astype(np.uint8)) for binary in masks]\n",
    "        for i in range(N):\n",
    "            c = centroids[i]\n",
    "            ymin, xmin = boxes[i][:2]\n",
    "            res.append([xmin+c[0], ymin+c[1]])\n",
    "        return(np.array(res) - res[0]) \n",
    "   \n",
    "    def pad_arrays(self, array, pad_amt=max_padding):\n",
    "    \n",
    "        pad_width = ((0, pad_amt - array.shape[0]), (0, pad_amt - array.shape[1]))\n",
    "\n",
    "        padded_array = np.pad(array, pad_width, mode='constant')\n",
    "        return padded_array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_sequences = self.cell_dict[idx]  #this is the first sequence of 10 cells\n",
    "        boxes = cell_sequences[0]\n",
    "        masks = cell_sequences[1]\n",
    "        patches = cell_sequences[2]\n",
    "\n",
    "\n",
    "        for cell_mask_num in np.arange(len(masks)): #should be sequence length (10) masks\n",
    "                \n",
    "                cell_time = np.array(masks[cell_mask_num], dtype=np.int32)\n",
    "                cell_time = np.where(cell_time >= 0, cell_time, 1)\n",
    "                cell_time = self.pad_arrays(cell_time)\n",
    "                masks[cell_mask_num] = cell_time\n",
    "                cell_time_patch = np.array(patches[cell_mask_num], dtype=np.int32)\n",
    "\n",
    "                cell_time_patch = self.pad_arrays(cell_time_patch)\n",
    "\n",
    "                patches[cell_mask_num] = cell_time_patch\n",
    "\n",
    "\n",
    "        centroids = self.get_centroids(boxes, masks)\n",
    "    \n",
    "\n",
    "        return centroids, masks, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c99cccaa-0e29-4973-9cf4-fca24e280e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "\n",
    "mip_video_files = [\n",
    "    ('mip', 3),\n",
    "    ('mip', 6),\n",
    "    ('mip', 9)\n",
    "]\n",
    "\n",
    "# dataset = CellBoxMaskPatch(mip_video_files, X) # file, S, T\n",
    "\n",
    "# train, val, test = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "# input_datasets = {}\n",
    "# input_datasets[\"train\"] = train\n",
    "# input_datasets[\"val\"] = val\n",
    "# input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6123bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import *\n",
    "# import mahotas #: Module(\"mahotas\")\n",
    "\n",
    "\n",
    "# def extract_traces_sparse(frames, masks, hist=2):\n",
    "#     bboxes, num_cells, areas = bounding_boxes(masks[0])\n",
    "#     vid_data = []\n",
    "#     for i in range(num_cells):\n",
    "#         #print(\"Extracting cell \", i)\n",
    "#         data = track_cells(i, frames, masks, padding=0, history_length=hist, verbose=False)\n",
    "#         vid_data.append(data)\n",
    "#     return(vid_data)\n",
    "\n",
    "# def shape_features(binary, feature_length=20, num_samples=180):\n",
    "#     def radial_distance(binary, theta):\n",
    "#         height, width = binary.shape\n",
    "#         center = [width // 2, height // 2]\n",
    "#         def test_r(r):\n",
    "#             x_test, y_test = center[0] + r*np.cos(theta), center[1] + r*np.sin(theta)\n",
    "#             if(x_test >= width or y_test > height or x_test < 0 or y_test < 0):\n",
    "#                 return(False)\n",
    "#             return(binary[int(y_test), int(x_test)])\n",
    "#         # calculate distance to the nearest pixel\n",
    "#         r = max(height, width)\n",
    "#         while(not test_r(r)): # start from edge come inside until hit cell\n",
    "#             r -= 1\n",
    "#         return(r)\n",
    "\n",
    "#     test_angles = np.linspace(0, 2*np.pi, num_samples)\n",
    "#     distances = np.array([radial_distance(binary, angle) for angle in test_angles])\n",
    "#     fft_coefficients = np.fft.rfft(distances)\n",
    "\n",
    "#     features = np.abs(fft_coefficients[:feature_length])\n",
    "#     features = features / np.sum(features)\n",
    "#     return(features, (distances, fft_coefficients))\n",
    "\n",
    "# def featurize(cell_data, index):\n",
    "#     image, binary = cell_data['patches'][index], cell_data['masks'][index].astype(np.uint8)\n",
    "#     zernike = mahotas.features.zernike_moments(binary, max(binary.shape)/2, degree=8)\n",
    "#     #zernike = zernike / zernike.sum()\n",
    "#     haralick = mahotas.features.haralick(image.astype(np.uint16)).mean(axis=0)\n",
    "#     #haralick = haralick / haralick.sum()\n",
    "#     shape, info = shape_features(binary, 20)\n",
    "#     #print(f\"Zernike: {zernike.shape}, Haralick: {haralick.shape}, Radial Shape: {shape.shape}\")\n",
    "#     return(np.concatenate([zernike, haralick, shape]))\n",
    "\n",
    "# class VideoDataProcessed:\n",
    "#     def __init__(self, files, sequence_length=5, channel=0):\n",
    "#         self.data = {}\n",
    "#         self.all_traces = []\n",
    "#         self.seq_length = sequence_length\n",
    "#         self.channel = channel\n",
    "#         self.videos = {}\n",
    "#         for category, num in files:\n",
    "#             print(f\"Loading in processed {num}\")\n",
    "#             assert category == 'processed', \"Can't load non processed file\"\n",
    "#             video = get_file(category, num)\n",
    "#             self.videos[num] = video\n",
    "#         self.num_vids = len(self.data)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_vids\n",
    "\n",
    "#     def extract_planes(self, num, zplanes, hist_length):\n",
    "#         for z in zplanes:\n",
    "#             self.extract_slice_traces(num, z, hist_length)\n",
    "    \n",
    "#     def extract_slice_traces(self, num, zPlane, hist_length=2):\n",
    "#         assert num in self.videos.keys(), f\"Video {num} not found\"\n",
    "        \n",
    "#         video = self.videos[num]\n",
    "#         frames, shp = video.read_image(C=self.channel, S=0, Z=zPlane)\n",
    "#         frames = scale_img(frames.squeeze())\n",
    "#         print(f\"vid {num} zplane {zPlane} with frames: {frames.shape}\")\n",
    "#         masks = binarize_video(frames)\n",
    "#         N = len(frames)\n",
    "#         s = 0\n",
    "#         for i in range(N // self.seq_length):\n",
    "#             print(f\"Extracting traces from {s}:{s+self.seq_length}\")\n",
    "#             data = extract_traces_sparse(frames[s:s+self.seq_length], masks[s:s+self.seq_length], hist=hist_length)\n",
    "#             s += self.seq_length\n",
    "#             self.all_traces = self.all_traces + data\n",
    "        \n",
    "#         if(N % self.seq_length > 0):\n",
    "#             data = extract_traces_sparse(frames[-1*self.seq_length:], masks[-1*self.seq_length:], hist=hist_length)\n",
    "#             self.all_traces = self.all_traces + data\n",
    "\n",
    "\n",
    "# class SparseMIPVideo:\n",
    "#     def __init__(self, files, sequence_length, hist_length=2):\n",
    "#         self.data = {}\n",
    "#         self.all_traces = []\n",
    "#         self.N = sequence_length\n",
    "#         for category, num in files:\n",
    "#             print(f\"Loading in MIP {num}\")\n",
    "#             assert category == 'mip', \"Can't load non Mip file\"\n",
    "#             video = get_file(category, num)\n",
    "#             frames, shp = video.read_image(C=0)\n",
    "#             frames = scale_img(frames.squeeze())\n",
    "#             print(f\"frames {num}: {frames.shape}\")\n",
    "#             masks = binarize_video(frames)\n",
    "\n",
    "#             print(f\"Finished loading frames and masks for MIP {num}\")\n",
    "\n",
    "#             N = len(frames)\n",
    "#             s = 0\n",
    "        \n",
    "#             for i in range(N // sequence_length):\n",
    "#                 print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "#                 data = extract_traces_sparse(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "#                 s += sequence_length\n",
    "#                 self.all_traces = self.all_traces + data\n",
    "            \n",
    "#             if(N % sequence_length > 0):\n",
    "#                 data = extract_traces_sparse(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "#                 self.all_traces = self.all_traces + data\n",
    "\n",
    "#     def featurize_traces(self):\n",
    "#         self.featurized_frames = []\n",
    "#         for i, trace in enumerate(self.all_traces):\n",
    "#             if(i % 100 == 0):\n",
    "#                 print(i)\n",
    "#             trajectory_features = np.array([featurize(trace, index) for index in range(5)])\n",
    "#             self.featurized_frames.append(trajectory_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e324da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in MIP 3\n",
      "mip\n",
      "Loading dicty_factin_pip3-03_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 90)}]\n",
      "frames 3: (90, 2048, 474)\n",
      "Loading in MIP 6\n",
      "mip\n",
      "Loading dicty_factin_pip3-06_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 241)}]\n",
      "frames 6: (241, 2048, 474)\n",
      "Loading in MIP 9\n",
      "mip\n",
      "Loading dicty_factin_pip3-09_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 241)}]\n",
      "frames 9: (241, 2048, 474)\n",
      "Extracting traces from 0:10\n",
      "Extracting traces from 10:20\n",
      "Extracting traces from 20:30\n",
      "Extracting traces from 30:40\n",
      "Extracting traces from 40:50\n",
      "Extracting traces from 50:60\n",
      "Extracting traces from 60:70\n",
      "Extracting traces from 70:80\n",
      "Extracting traces from 80:90\n",
      "Extracting traces from 0:10\n",
      "Extracting traces from 10:20\n",
      "Extracting traces from 20:30\n",
      "Extracting traces from 30:40\n",
      "Extracting traces from 40:50\n",
      "Extracting traces from 50:60\n",
      "Extracting traces from 60:70\n",
      "Extracting traces from 70:80\n",
      "Extracting traces from 80:90\n",
      "Extracting traces from 90:100\n",
      "Extracting traces from 100:110\n",
      "Extracting traces from 110:120\n",
      "Extracting traces from 120:130\n",
      "Extracting traces from 130:140\n",
      "Extracting traces from 140:150\n",
      "Extracting traces from 150:160\n",
      "Extracting traces from 160:170\n",
      "Extracting traces from 170:180\n",
      "Extracting traces from 180:190\n",
      "Extracting traces from 190:200\n",
      "Extracting traces from 200:210\n",
      "Extracting traces from 210:220\n",
      "Extracting traces from 220:230\n",
      "Extracting traces from 230:240\n",
      "Extracting traces from 0:10\n",
      "Extracting traces from 10:20\n",
      "Extracting traces from 20:30\n",
      "Extracting traces from 30:40\n",
      "Extracting traces from 40:50\n",
      "Extracting traces from 50:60\n",
      "Extracting traces from 60:70\n",
      "Extracting traces from 70:80\n",
      "Extracting traces from 80:90\n",
      "Extracting traces from 90:100\n",
      "Extracting traces from 100:110\n",
      "Extracting traces from 110:120\n",
      "Extracting traces from 120:130\n",
      "Extracting traces from 130:140\n",
      "Extracting traces from 140:150\n",
      "Extracting traces from 150:160\n",
      "Extracting traces from 160:170\n",
      "Extracting traces from 170:180\n",
      "Extracting traces from 180:190\n",
      "Extracting traces from 190:200\n",
      "Extracting traces from 200:210\n",
      "Extracting traces from 210:220\n",
      "Extracting traces from 220:230\n",
      "Extracting traces from 230:240\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "mip_video_files = [\n",
    "    ('mip', 3),\n",
    "    ('mip', 6),\n",
    "    ('mip', 9)\n",
    "]\n",
    "\n",
    "dataset = CellBoxMaskPatch(mip_video_files, X) # file, S, T\n",
    "\n",
    "train, eval, test = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "input_datasets = {}\n",
    "input_datasets[\"train\"] = train\n",
    "input_datasets[\"eval\"] = eval\n",
    "input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f44453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import random_split\n",
    "# import VideoLoaders\n",
    "\n",
    "# X = 10\n",
    "# processed_video_files = [\n",
    "#     ('processed', 3),\n",
    "# ]\n",
    "# processed_dataset = VideoLoaders.VideoDataProcessed(processed_video_files)\n",
    "# processed_dataset.extract_slice_traces(3, 50)\n",
    "\n",
    "# # train, val, test = random_split(processed_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "# # input_datasets = {}\n",
    "# # input_datasets[\"train\"] = train\n",
    "# # input_datasets[\"val\"] = val\n",
    "# # input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "314d04ff-2a4d-4226-9838-026a10d37ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, mode_box, mode_mask, mode_patch):\n",
    "    current_centroids = [b[0] for b in batch]\n",
    "    current_masks = [b[1] for b in batch]\n",
    "    current_patches = [b[2] for b in batch]\n",
    "\n",
    "    current_centroids = torch.tensor(np.stack(current_centroids), dtype=torch.float32)\n",
    "    current_masks = torch.tensor(np.stack(current_masks), dtype=torch.long)\n",
    "    current_patches = torch.tensor(np.stack(current_patches), dtype=torch.long)\n",
    "\n",
    "    current_patches = current_patches.reshape([len(batch), 10, max_padding*max_padding])\n",
    "    current_masks = current_masks.reshape([len(batch), 10, max_padding*max_padding])\n",
    "\n",
    "    selected_tensors = []\n",
    "    if mode_box:\n",
    "        selected_tensors.append(current_centroids)\n",
    "    if mode_mask:\n",
    "        selected_tensors.append(current_masks)\n",
    "    if mode_patch:\n",
    "        selected_tensors.append(current_patches)\n",
    "\n",
    "    combined_tensor = torch.cat(selected_tensors, dim=-1)\n",
    "\n",
    "    # Cast the combined tensor to torch.float32\n",
    "    combined_tensor = combined_tensor.to(torch.float32)\n",
    "\n",
    "    return combined_tensor, current_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3f83b05-0e73-4f12-8ca1-a207a201b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box is actually a box surrounding the cell\n",
    "# mask is the values of the cell\n",
    "# patch is fluorescence\n",
    "mode_box = False\n",
    "mode_mask = False\n",
    "mode_patch = True\n",
    "\n",
    "input_size = 0\n",
    "if mode_box:\n",
    "    input_size+=2\n",
    "if mode_mask:\n",
    "    input_size+=max_padding*max_padding\n",
    "if mode_patch:\n",
    "    input_size+=max_padding*max_padding\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['test'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['eval'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['eval'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45f225f6-9e7f-4b15-bd8b-34d63d3b8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([4, 10, 90000]) Centroids torch.Size([4, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders['eval']:\n",
    "\n",
    "    print(\"Input:\", batch[0].shape, \"Centroids\", batch[1].shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ab035",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "568655b8-8ed2-4097-829f-0423b5afdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d191cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9dbae0a-4601-4383-a79d-596d32dab831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True) #stacking 2 LSTMs\n",
    "        # hidden out output\n",
    "        #  2 bc x y centroid\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "    def forward(self, input):\n",
    "        #h_0: tensor of shape (D∗num_layers,N,Hout​) containing the initial hidden\n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).to(input.device)\n",
    "        c0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).to(input.device)\n",
    "\n",
    "       \n",
    "#         out, _ = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "        out, _ = self.lstm(input, (h0, c0))\n",
    "\n",
    "        out = self.fc(out)\n",
    "        final = out[:,-1,:]\n",
    "        out = torch.sigmoid(final) * max_padding\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(ImprovedLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 2) \n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional LSTM\n",
    "        c0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional LSTM\n",
    "\n",
    "        out, _ = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14627efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(MultiLayerLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  \n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  \n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  \n",
    "        c0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  \n",
    "\n",
    "        # out, _ = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "        out, _ = self.lstm(input, (h0, c0))\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "\n",
    "        out = out.to(torch.float32)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ab2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(MultiLayerGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  \n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  \n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  \n",
    "\n",
    "        out, _ = self.gru(input, h0.detach())\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "\n",
    "        out = out.to(torch.float32)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerComplexGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=2, dropout=0.2):\n",
    "        super(MultiLayerComplexGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru1 = nn.GRU(input_size, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim * 2)  \n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)  \n",
    "        self.fc3 = nn.Linear(hidden_dim, 2) \n",
    "\n",
    "    def forward(self, input):\n",
    "        # Init\n",
    "        h0_1 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional GRU\n",
    "        h0_2 = torch.zeros(self.num_layers * 2, input.size(0), self.hidden_dim).to(input.device)  # Adjusted for bidirectional GRU\n",
    "\n",
    "        out, _ = self.gru1(input, h0_1.detach())\n",
    "        \n",
    "        out, _ = self.gru2(out, h0_2.detach())\n",
    "\n",
    "        # Predict to mad padding\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out) * max_padding\n",
    "\n",
    "        out = out.to(torch.float32)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2a95b-40a9-421d-a3f2-f8218a4a7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrivialLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=2, hidden_size=1000, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(1000, 2)  # To ensure the output size matches the input size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        output, _ = self.lstm(x)\n",
    "        # Linear layer to match the output size to input size\n",
    "        output = self.linear(output)\n",
    "        return output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb34bf3-5281-4f7a-8f26-020550b01dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        #Flatten the input from (4, 9, 2) to (4, 18)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(810000, 500) \n",
    "        self.fc2 = nn.Linear(500,1000)\n",
    "        self.fc3 = nn.Linear(1000,500)\n",
    "        self.fc4 = nn.Linear(500, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) \n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8438fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 500) \n",
    "        self.fc2 = nn.Linear(500,1000)\n",
    "        self.fc3 = nn.Linear(1000,500)\n",
    "        self.fc4 = nn.Linear(500, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "928c7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class SimpleCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         # Adjust the input size of fc1 based on the output size after convolutions and pooling\n",
    "#         self.fc1 = nn.Linear(32 * 5 * 5, 500) \n",
    "#         self.fc2 = nn.Linear(500,1000)\n",
    "#         self.fc3 = nn.Linear(1000,500)\n",
    "#         self.fc4 = nn.Linear(500, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(\"zero\", x.shape)\n",
    "#         x = self.conv1(x)\n",
    "#         print(\"after first\", x.shape)\n",
    "#         x = torch.relu(x)\n",
    "#         print(\"after secon\", x.shape)\n",
    "#         x = self.pool(x)\n",
    "#         print(\"after third\", x.shape)\n",
    "#         x = self.conv2(x)\n",
    "#         print(\"after fourth\", x.shape)\n",
    "#         x = torch.relu(x)\n",
    "#         print(\"after fifth\", x.shape)\n",
    "#         x = self.pool(x)\n",
    "#         print(\"after six\", x.shape)\n",
    "#         x = self.flatten(x)\n",
    "#         print(\"after seven\", x.shape)\n",
    "#         x = (self.fc1(x)) \n",
    "#         print(\"after eight\", x.shape)\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         print(\"after nine\", x.shape)\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         print(\"after ten\", x.shape)\n",
    "#         x = self.fc4(x)  \n",
    "#         print(\"after last fc4\", x.shape)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3aa1d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleCNN(nn.Module):\n",
    "\n",
    "# # inputs torch.Size([4, 10, 90000])\n",
    "# # outputs torch.Size([4, 10, 2])\n",
    "\n",
    "# # first torch.Size([4, 9, 90000])\n",
    "# # second torch.Size([4, 16, 90000])\n",
    "# # third torch.Size([4, 4, 90000])\n",
    "# # four torch.Size([16, 90000])\n",
    "# # five torch.Size([16, 128])\n",
    "# # six torch.Size([16, 2])\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=9, out_channels=16, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=16, out_channels=4, kernel_size=3, padding=1)  \n",
    "#         self.conv3 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=3, padding=1)  \n",
    "#         self.fc1 = nn.Linear(4 * 90000 // 4, 128)  \n",
    "#         self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(\"first\", x.shape)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         print(\"second\", x.shape)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         print(\"third\", x.shape)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         print(\"four\", x.shape)\n",
    "#         x = x.view(-1, 16 * 90000 // 4)  \n",
    "#         print(\"five\", x.shape)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         # print(\"five\", x.shape)\n",
    "#         x = self.fc2(x)\n",
    "#         # print(\"six\", x.shape)\n",
    "#         return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0bbb82b3-20cd-44eb-9f9f-43a55697cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 2000\n",
    "num_layers = 2\n",
    "epochs = 1000\n",
    "sequence_length = 10 #how many frames we process per input\n",
    "\n",
    "# model = LSTM(input_size, hidden_size, num_layers)\n",
    "model = SimpleCNN()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# dummy_input_data = torch.randn(batch_size, 10, input_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in dataloaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, outputs = batch[0], batch[1]\n",
    "        # print(\"inputs\", inputs.shape)\n",
    "        # print(\"outputs\", outputs.shape)\n",
    "        # shape is (batch_size * frames * input)\n",
    "        # print(inputs[:, sequence_length-1:sequence_length, :].shape)\n",
    "#         print(outputs[:,-1,:].shape)\n",
    "        inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "        pred = model(inputs[:, :sequence_length-1, :])\n",
    "        # print(\"final prediction\", pred.shape, \"should be\", outputs[:,-1,:].shape)\n",
    "        # print(inputs[:, sequence_length-1:sequence_length, :].shape)\n",
    "        # print(pred.shape, outputs[:,-1,:].shape)\n",
    "        # print(f\"pred: {pred}\")\n",
    "        # print(f\"outputs: {outputs.data[:,-1,:]}\")\n",
    "        # total_correct += torch.sum(torch.eq(pred, outputs[:,-1,:]))\n",
    "        loss = criterion(pred, outputs[:,-1,:])\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # print(f\"training loss: {total_loss / len(dataloaders['train'])}, training accuracy: {total_correct / len(dataloaders['eval'])}\")\n",
    "    print(f\"training loss: {total_loss / len(dataloaders['train'])}\")\n",
    "    return model\n",
    "\n",
    "def eval():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloaders['eval']:\n",
    "            inputs, outputs = batch[0], batch[1]\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            pred = model(inputs[:, :sequence_length-1, :])\n",
    "            # print(pred.shape)\n",
    "            # print(outputs.shape)\n",
    "#             print(pred,outputs[:,-1,:])\n",
    "            \n",
    "            loss = criterion(pred, outputs[:,-1,:])\n",
    "            total_loss += loss.item()\n",
    "        # X = np.array(outputs[0, :, 0])\n",
    "        # Y = np.array(outputs[0, :, 1])\n",
    "#         plt.plot(X[:9], Y[:9], marker='o', linestyle='-')\n",
    "#         plt.scatter(X[-1], Y[-1], color='orange', label='ground truth')\n",
    "#         plt.scatter(pred[0, -1, 0],pred[0, -1, 1], color='red', label='pred')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    print(f\"validation loss: {total_loss / len(dataloaders['eval'])}\")\n",
    "    return total_loss / len(dataloaders['eval'])\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        train()\n",
    "        curr_acc = eval()\n",
    "        if curr_acc > best_acc:\n",
    "            best_acc = curr_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "32d560a2-0af0-4374-bc65-739e0d8713e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "zero torch.Size([4, 9, 90000])\n",
      "after first torch.Size([16, 9, 90000])\n",
      "after secon torch.Size([16, 9, 90000])\n",
      "after third torch.Size([16, 4, 45000])\n",
      "after fourth torch.Size([32, 4, 45000])\n",
      "after fifth torch.Size([32, 4, 45000])\n",
      "after six torch.Size([32, 2, 22500])\n",
      "after seven torch.Size([32, 45000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x45000 and 800x500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model()\n",
      "Cell \u001b[0;32mIn[158], line 75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m---> 75\u001b[0m     train()\n\u001b[1;32m     76\u001b[0m     curr_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_acc \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Cell \u001b[0;32mIn[158], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# print(\"inputs\", inputs.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# print(\"outputs\", outputs.shape)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# shape is (batch_size * frames * input)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# print(inputs[:, sequence_length-1:sequence_length, :].shape)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#         print(outputs[:,-1,:].shape)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         inputs, outputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), outputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 30\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(inputs[:, :sequence_length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# print(\"final prediction\", pred.shape, \"should be\", outputs[:,-1,:].shape)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# print(inputs[:, sequence_length-1:sequence_length, :].shape)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# print(pred.shape, outputs[:,-1,:].shape)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# print(f\"pred: {pred}\")\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# print(f\"outputs: {outputs.data[:,-1,:]}\")\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# total_correct += torch.sum(torch.eq(pred, outputs[:,-1,:]))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(pred, outputs[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[157], line 33\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter seven\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)) \n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter eight\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x45000 and 800x500)"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146ac4d-71f3-4f35-8000-175fa16b7fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752460f6-f7a6-4b21-978d-d494889ad2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
