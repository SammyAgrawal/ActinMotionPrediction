{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f45704a9310>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aicspylibczi import CziFile\n",
    "import czifile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.animation as animation\n",
    "import IPython\n",
    "from IPython.display import HTML\n",
    "import ffmpeg\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "import ffmpeg\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from PIL import Image\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = [\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones([100, 100])],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones([100, 100])]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "fake_data[0][\"masks\"][1].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get datasets\n",
    "#one item is one cell\n",
    "#data = {\"patches\" : [], 'boxes' : [], \"masks\" : []} \n",
    "\n",
    "# Create a lookup table to go between label name and index\n",
    "\n",
    "class_names = \"patches\", \"boxes\", \"masks\"\n",
    "num_classes = len(class_names)\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for idx, label in enumerate(class_names):\n",
    "    id2label[str(idx)] = label\n",
    "    label2id[label] = str(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_shape = [100,100]\n",
    "np.zeros((box_shape[0], box_shape[1])).flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/mnt/datadisk/\"\n",
    "subdirs = ['FactinProcessed', 'FactinMIP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicty_factin_pip3-09_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 241)}]\n"
     ]
    }
   ],
   "source": [
    "video = get_file('mip', 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('T', 241), ('C', 1), ('Y', 2048), ('X', 474)]\n"
     ]
    }
   ],
   "source": [
    "frames, shp = video.read_image(C=0)\n",
    "print(shp)\n",
    "frames = scale_img(frames.squeeze())\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_frames(frames):\n",
    "    fig = plt.figure()\n",
    "    im = plt.imshow(frames[0])\n",
    "    \n",
    "    plt.close() # this is required to not display the generated image\n",
    "    \n",
    "    def init():\n",
    "        im.set_data(frames[0])\n",
    "    \n",
    "    def animate(i):\n",
    "        im.set_data(frames[i])\n",
    "        return im\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(frames),\n",
    "                                   interval=200)\n",
    "    \n",
    "    HTML(anim.to_html5_video())\n",
    "    plt.show()\n",
    "    return(anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anim = animate_frames(frames)\n",
    "#HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = get_file(\"processed\", 3)\n",
    "\n",
    "#video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_video_files = [\n",
    "    (\"processed\", 3),\n",
    "    ('processed', 6),\n",
    "    ('processed', 9),\n",
    "]\n",
    "mip_video_files = [\n",
    "    ('mip', 3),\n",
    "    ('mip', 6),\n",
    "    ('mip', 9),\n",
    "]\n",
    "\n",
    "new_video_files = [ # these are weird don't use them\n",
    "    ('new', 2), \n",
    "    ('new', 3),\n",
    "    ('new', 4),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_traces(frames, masks, hist=2):\n",
    "    bboxes, num_cells, areas = bounding_boxes(masks[0])\n",
    "    vid_data = []\n",
    "    videos_for_checking = []\n",
    "    for i in range(num_cells):\n",
    "        print(\"Extracting cell \", i)\n",
    "        data = track_cells(i, frames, masks, padding=0, history_length=hist, verbose=False)\n",
    "        vid_data.append(data)\n",
    "        videos_for_checking.append(np.array(visualize_cell_tracker(frames, data)))\n",
    "\n",
    "    return(vid_data, videos_for_checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataMIP:\n",
    "    def __init__(self, files):\n",
    "        self.data = {\n",
    "        }\n",
    "        \n",
    "        for category, num in files:\n",
    "            print(f\"Loading in MIP {num}\")\n",
    "            assert category == 'mip', \"Can't load non Mip file\"\n",
    "            file = {}\n",
    "            file['video'] = get_file(category, num)\n",
    "            \n",
    "            frames, shp = file['video'].read_image(C=0)\n",
    "            frames = scale_img(frames.squeeze())\n",
    "            file['frames'] = frames\n",
    "            print(f\"frames {num}: {frames.shape}\")\n",
    "            file['masks'] = binarize_video(frames)\n",
    "\n",
    "            self.data[num] = file\n",
    "\n",
    "\n",
    "    def extract_all_traces(self, file_num, sequence_length, hist_length=2):\n",
    "        # hist length is how many frames of history\n",
    "        frames, masks = self.data[file_num]['frames'], self.data[file_num]['masks']\n",
    "        N = len(frames)\n",
    "        s = 0\n",
    "        all_traces = []\n",
    "        all_videos = []\n",
    "        for i in range(N // sequence_length):\n",
    "            print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "            data, videos = extract_traces(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "            s += sequence_length\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos\n",
    "        \n",
    "        if(N % sequence_length > 0):\n",
    "            data, videos = extract_traces(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos\n",
    "\n",
    "        self.data[file_num]['traces'] = all_traces\n",
    "        self.data[file_num]['trace_videos'] = all_videos\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in MIP 3\n",
      "Loading dicty_factin_pip3-03_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 90)}]\n",
      "frames 3: (90, 2048, 474)\n",
      "Loading in MIP 6\n",
      "Loading dicty_factin_pip3-06_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 241)}]\n",
      "frames 6: (241, 2048, 474)\n",
      "Loading in MIP 9\n",
      "Loading dicty_factin_pip3-09_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 241)}]\n",
      "frames 9: (241, 2048, 474)\n",
      "CPU times: user 31.8 s, sys: 5.65 s, total: 37.5 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vids = VideoDataMIP(mip_video_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting traces from 0:10\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting traces from 10:20\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 20:30\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 30:40\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 40:50\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 50:60\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 60:70\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting traces from 70:80\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting traces from 80:90\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n"
     ]
    }
   ],
   "source": [
    "vids.extract_all_traces(3, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvids\u001b[49m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraces\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vids' is not defined"
     ]
    }
   ],
   "source": [
    "vids.data[3]['traces'][0]['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vids.data[3]['traces'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAppG1kYXQAAAKgBgX//5zcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU3IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENv\n",
       "cHlsZWZ0IDIwMDMtMjAxOCAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9w\n",
       "dGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1o\n",
       "ZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2\n",
       "IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0\n",
       "X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTE1IGxvb2thaGVhZF90aHJlYWRz\n",
       "PTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50ZXJsYWNlZD0wIGJsdXJheV9j\n",
       "b21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBiX3B5cmFtaWQ9MiBiX2FkYXB0\n",
       "PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29wPTAgd2VpZ2h0cD0yIGtleWlu\n",
       "dD0yNTAga2V5aW50X21pbj01IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhl\n",
       "YWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02\n",
       "OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAATJmWIhAAT//73sY+BTcgADZc6\n",
       "inof4RWx9JBRerHZoGTqAAADAAADAAADAigXteKQMJledeAAABkwAzB9H+IznhwAEYETXOr2UCUk\n",
       "LTAsPYUI9TkofchozT8e8ioDhlnJjLjrkhtFNx0AmuWCNS2UFO/U5TA9Ra3NwklzqrTP70XFH9mz\n",
       "pL0P020bB2JFkrRp83enhyCB7Tseb3Q4Aholq/TbjpW15CZsSyeVNMITFnj+Fbb/mcf5zOQMdW9I\n",
       "nruYwFesmsHP+611cY0a3jCP+Sv8sWkkhpyfCU6ceW+X6CUrKLtdkXABOa++kf/zZ75yHUgJUYvq\n",
       "DA0jk7W8xqiRYmvOXHKStXHGH1nRKJ3KzeAV0GRiAfyI9NhouVUMQnlhOYTmraXLuZuT/e///itl\n",
       "TEG/24hsYIYpjHacVS+FyxMRRKIq53GH3rtxkUUEQxthviDTwx/jLhoj19u8vuX0a9FW7gNnlbNO\n",
       "oLZIOkANpG1OcpX85DHIhl+j8Wg9u5ofA0T2HBATilqT0r2mkPMBIDOU5q8QPPDJm94wQ+NaZ4Uv\n",
       "YT4xUNqEPK+ZBVED9QyWk8M8D7lfETHGh7FbSv5i3JcPhCYhnPOds2cshdQE1042CsqerMfeUKKJ\n",
       "00h4Beuj+VlTdfvLCwgSIeooJ0HOLNXlTDRPDkxTeieA5xM+9mJdh38xUOjV+LG+eoIGFaiOXnU+\n",
       "F0qDRLlEmynaSvY+fZ94geQZ1DJf3P7IfWqK18mi4Ih7LyflYVXUnQAWmtuOL8Tc7qqhLodETfXT\n",
       "OhMUcrmP22kr1g7H6r+3q8sD3al/1Wl3kTt8MuEBGsOp+7pbK/tZ/GMr9EUth017s6dbpyuUBxZN\n",
       "qSIfyO24PrD08zUGbzPOjPKXbQ4PmvnApClUcxw5L8rNLmMhi7n7vVqEKXzm9jdSwKqTJ4ZQeDpN\n",
       "nJtnEueN0wALetj4Y0PTa+hISSb1ELDV5t6Cp1zobxKNtR+Q2k+MPdg/7GdeB7sVw+K587dpPe+m\n",
       "whAUmXdLuGhD/GrwCpotW+UZ35lQMKU+XHRdoM3kZFT7hEjEBQGvqSMHr1MGvG+nN9FYfMxWe3KF\n",
       "nzcJJ33H/jnsXchDLGExgwlcxk369UKBv3E4zfhVOiv93BNET8uAR8kbqWMig5oLH80dNtG0QDtP\n",
       "GWQcECbS9UN8ijStFAaXueEQsTA4m/3QvsEBuFQ7owlEIenyfaCFPuQzjU5DMx7dfcBBXK3g+m1o\n",
       "HSnDDDsk7lEHOxEQ+5mMW0rpuxCaQjOo/n7MDexr3lovJmvyvBINk5xjtcPkgm8V1KqvJtFE5M9b\n",
       "x0vfNBaabYSAebkRRiuru/Gt8j8v5aR07RTxkcTjFW2CYKSE/qKit3PR/YS93srC2KefV9KsmR/e\n",
       "kL6GHAb86KXKM1g/8nMsibZWFF0TqY+Pks1qSdlUtpERtVLvg9tDzd/zqsrhKm/xWzJxdCoCuHfz\n",
       "+cSWQUXRRROB2oJSxmcjn+WaNliSetTU+VKTYGhsbsdayIjtgCw8RsvzV3bAgiNzT7l6yJCRWtB7\n",
       "O0WtMiVZmCnTHKRWuRpO+XH5V+eCEJjEpD9ex4kB/9wRxbuzSC44NM9LGEZQH8Mr5/sthnKuycsf\n",
       "cf7bj0ncFIIAUIWuJHu8IJM+d9oYny1LU7MmFUtOJuNiUTuzuKy/06d2HctsjjaBPqQOX/RW547Y\n",
       "bW+lFuknwtLZLrUREBicO6IfBzBWkDRIuTkbBRSgYOQ0tHZcmU52yl5DGtVqvWhmPRENTxQTsugA\n",
       "eZsGeg3bm+YS/qLmxFkTwbyninxybKaTY2D5FcNFQd/IDWcDoe7Wasd7QjFxPqhQITDT6eZvgz8g\n",
       "GgOM8l5WX9+aaVC+0/iPA/btSWtEIxICKPpYCno7T3wi86Qk4iYWXVtg/RbyjlvEFy3qpmiESnxg\n",
       "qN5rQSFjN4TlT78zUI6xQr+pALWZ4SZAx4q63AoK0zHeNnCZjREkcRfvoV8ZCAIoo8HWqsPUeF1s\n",
       "H0hAA5BbPr+OotYy0yS6eQPeqjmk31+bNXvAn3sw+ZaTXbGAcsByK0M/h+SEpDPfF/7CTZ5PmBbB\n",
       "vmvb0rIw6CKuoqilEEsABZ0O1EvvH6+C0krqCjOSIdl+qjS4k5anUXT09uBenHUIEByg/6+L6324\n",
       "Dp8cUKbX6z+XomgdTlwpkVPeFoV1Q1ldZOfIN5FAfAfrP4eMMNqwYFdpaseJpU6ckjgYLqtlreb4\n",
       "IYVv0R1RFqeIGSsQWMSh18H7tSgbfi5zdLn+dGibrPmRu9WkzT86xX929+nLDd+I1XjSj/O9IVPG\n",
       "64WjmkYdml5bakInawsZpEetodxyVtaQd12xt0csVEx1F5CUN+2bZyMxX8E64Zx/c0rsKVyK+V1E\n",
       "JGWpJlTx+LWOKUKGk9O97YcPxmIaSJULw3tdtoQtcByVdrgpXwcm3C1t/AIdM7gR1mUVggDpbfIH\n",
       "L0kZ+iPFaZqhwB3qeLphRC1BrAhEzvNdAHwxC7FxFKOGz3dhZKwkoCmCgzhikaLP5CVZpSBv6vy8\n",
       "I87obkx+sbAr+QxrTlmmmUEa11wgG1ovKZCXJLWokwXzVaBiqm+AvPDs3BwpucbNX9wsex2GXZ1w\n",
       "B/fKx5AoPtfUlZ1BAA2yRaTkvQIZ4115+tREFevl4MAqjM+KL+d3hXOvSa3SeqhR+JpKz3YUjp58\n",
       "TIC5U/O22HdV5L7tKhVSC0l91z4KlPz3ZmAJABMLH6ZvSC/15rZDX6xEXe0cqXsVQjUoy2FTJhfj\n",
       "779YUAkkGu3BIeOTVL/rap0wRXnNYJkDyZCgI2+TB+0E7rOZ9Lax+Qs7z7+02ZmIGdlxus3l/FLg\n",
       "L+GPbEA+p0dCZJo1CrMH5l7FWoa740ZEqG2Z1De/fvLv5wSd2Y/JKuaVtVd+mu9RP0F0DvFvC9ld\n",
       "6/qDxeoq3ce3rqgRLKg+HIKX0nhg8tXnnjVCZ6gbhNnRB+qWXBGXA/ZBzL2A4vecUqE2RrQNI4Vl\n",
       "9mqHn40AckiMlDVKnCmtXqmo6zNDho19R2pcRqFWNqfJ6nzivtI+Or7cDcfGBXTd63TAXHVkugTA\n",
       "ANFiTE5OWvh9LNZJT2Mipv+30SNJQTwaxG+09H6PrCBnZyI0GsS3D90Q8DcigDuaShMFaW1V67fR\n",
       "ZkR1f4qZvhi6cGGZi7pGVIr8Su68fftHiwQD5XU6JA7qg+Tn6Rc21dJ4ErVTSFhxm3uxEmKcAFK9\n",
       "eszJv1WOuOkiQyEylOlU2VXVWn5vor36vkeO5LD1aZhEYlMmE7aLCfZqeN4viFM9K52z/iAAK9s3\n",
       "nXFMGBGM1YP+47PR8uYEfb1Z1fuHEeNt9lZOPVXBjK0ljLlunsiObxLV3KIVkAeES8K2+FZfzBHv\n",
       "1J6FANzxN5GUJOS/0EpzNOY9ThowYSNjRVGEbZTqbxnpPv3E/2ziVqstMe9FcJ+fhZBLqrkBqggz\n",
       "Ruky1RSGOTif0NhepsBFX/jcQzMCO/hdlrsmg7vk+GH5cZOITCdARe/MypYIhvLL+krfn1pCgz8K\n",
       "JyPnFvuEQiP2Uq1zgalE7v9Dy5BNt4Ahm43032pWUnGY/7qITOVQAHpgou7ASF/e9LwJyMTVFoUl\n",
       "dsycfCmPak+ToRO9e3t6Zj5lCo6LwtlnsaxlBv7AbeZlhTKXjCZK9E8ziQ4JMwKIODY0P0qwtn7n\n",
       "WOTW2b5DZRnIuNeNG+GoiEefZDKyzyA7gIX8xbWP/6q/D51NSpspB9MGjTiLE1cNaBL7d0ywLCh2\n",
       "wUguvFEQx0JBv8AFrhRB1bNPPJFvXjaJ+78diLh7MFZYj1tvfutvl37LGPuQ9mIsxXXPfOUxBVyK\n",
       "ikf1llWEiSrzdEY9b7sATGM/mzEfGxwdg3YLwiYGEKSDVI6XqtjCpejVfPhALCnYi7Yu0t7li9i5\n",
       "LM+Xmlgwb063e0XFduQi/cIAiVwgI4xTEuBkz57rQRAqORrId0PxapTjdJFQvFYZY42KOpagEwlK\n",
       "PVWNeFB7IBF9VONZKGB9nqQiYeYfUUWES+LOmOuaQa6gns/Y+5J4e0UFu7ZLfTNSAP4CTJQQZ3bt\n",
       "BOsJVNLDaRel58AZXChtv/vGZ9N+GfCR/aaIV+OsAL6I6rYjia5QfiZFlyPXkSSfHxKLKJKqkxBd\n",
       "Yc3p3xinnoavoWfsNy9UKjJZEwiraIUbK1SWl+1hIkKcsWl7XDnKtLNKh0ER2bThgusfRPSrq3eI\n",
       "rSfkeYC/SyKJ+/CeXLx1VB2pKTUjSqfHfa0F7SdOet24/3fH5amVJb2uIqxt69jEltEn7fZXCfIu\n",
       "DXyZ5umYpJpzSrF2fWq/80mv3lZTtydsSVslpkbdTgQzKGvvsAHi+m3mpuB3Jp3fagxCKDyupFBY\n",
       "pdaUJZNtgBF5pnpmB4Zlnoksfe4LiLKt+tVkB3fOX61a4CUZbHlqJpAQINxBC3oaqAtCTrIew+vW\n",
       "HeA+r4BpMtbqxYbyZkcNq3jcFFiyZ6mt3brXAe+LLmEVItZkkbF1nomgeQlG0mWHgAIpD8FIbX37\n",
       "4G/k6P/dKw94hf+iefixkiIZErnjm6ZSCNfq9d0Ih9Lh/y0EE8+smhPPXeaMayYd4NLj15X5DIFQ\n",
       "0sMu30pSxPBaIMUmBrj9eGTFPTXD86oUoMN5ou6MCKAYoBG6VpBGT2EVutwibd1gBVVpUIteykx4\n",
       "utlu8XcGu947LkgNsX2WygJYGSt/wgAdWfZWKbFwT9sF0tTAkz/KjLwjmfBactJAclz6H7hdpa1L\n",
       "t7phMIZ2TClN0Usz7vTsn7kdAMNySAYF3QKoNQQlyZ9vNaQCInA9ZLbVAUoqQBLocPZJprUZ/s0E\n",
       "lUnxOmf4AtuGIPkp10c+wgOs/B3lh9Mno2N+gsEOaBj6Bc5h4AZ6v/CRJOJXyIYV/mw9bJRkfGC1\n",
       "0vU9lfpZ1bfaiZy6WEELcZmuj8IlsnA16pqnTqs6M9uoPpC009rcgWxezmpLnhz1rYv+uGqjfSlJ\n",
       "Qmyv1bTJEubaI2px1b/qiKqww5wRm5YkYwySHWL/EfqPyZhbvMUKqZghsR3TyrksC6vVDFKYicwl\n",
       "K0TWXWiLhZgPhoNDUierxF1rebLqHY+LzAUZjFgndzkT/1pvgAxgE8QQ6X66E0MzapfnT4THIVfV\n",
       "wk7fuW32TJl9yKAzCUuuj3d5ee98Os0JRGvH+0WW7EVZvObaqezPf0bkZAfV2BaYkJwsnELM0d+E\n",
       "HSfBf6GT+6sMYrstFAAu/jHduZkLWVeTIGKG6jI5Ai8fzPT057Ub0aaJtW3QptP961M8oc+KPe8E\n",
       "0Ol8MtfYeNsafUcHNfGZgAd1GQjPkkgkBAJ9aU9zya6I7bdk7ESCWbPV7f2ERWpbmYpEFuOTqV1u\n",
       "p6RvCd8/COpr+a5LECGRPFCpyXKMQObsXZFzWQDO/WTr9jnOX7tDHj9V/gpp2Mvyece1usNkhjWP\n",
       "nhFnOEqZXprl3b9hO0aNtZ5dP1aQerjkiz5zOge2+GHRwHc+NSdVGuBx3ZBUYk08Ob8IsIklDMRb\n",
       "PClovYGNjOtHcy6JPUua+dQxWj607YAAC58EYxFTuf3eeseWoQdpBgIUWrdddk+EWBKTOAvNzYwK\n",
       "kerXDwk+UIJwZakd7AeWOzZuvR5ZEXo2rpTSFmFT0hRG3s7E8FWWUj1lPnVNAnwob/uOYf+7V8Uy\n",
       "rTzMJ9ofyeaeg+NtIsFSR5lIAxrXcZMoy1oGhc6gA0UWwDY6OX2mS/gKOH+HUckRc/8IOfYqwDrS\n",
       "IQQpLYRF574vo6UZUcfbkr5nlaGSuxTxN/GOyANU5Sf2aM6lSxSq6EHu6vRfMhmPAgRsowWIRQNg\n",
       "tDI9eMA+i8otElXw48EApitbK4fEqdOKhox230Gzfbi320ZjBtXX4Kg0+Yincps7zBAQIV3I/NKJ\n",
       "HcdO+9SUjH+nvGf83coFCPb3DjoGxaJopVouUW8DKWaeRVvVblj/rOj3ia+QAZufKwoAbtPKKAIL\n",
       "JzIeD4DeG+LTAHzuK7cOpN0KU07j3jEt57eX7R7P3qNLURaYiLrbvZfZgINOofmHoyyKBYfwZNXj\n",
       "bTOL26Tpfoj8AAADAGL/ldzNl0EyJSyq5VUu4TJuybj8PziD9hoUHK8GpT00CHhMsi4oYT18yvN2\n",
       "3BnmRK0ixQxSDMZt/a79AcSXVp/vkIwEtF8mQs1WyCdXD/SgBVXwZoe+a9b/RA8+RN3QDXiphY2w\n",
       "+h+UDN8aqeTFq9MO/ECvaq2YAspKNTMHNn5fIaKxRZjpHyMyo0JiQdsQty5Wt8QBKlGTIOc0zr5X\n",
       "uO8/GpLjng4p5KgAOQ/+9Jll2iJ8fM1sVe7euzsyrwaWdc28uQ1ZTJKFMb+ov+AROjXxCqprrge/\n",
       "KCjszE4z5bgJJ/CAU/EV1bCxm1PGKo9d+I43aAC/vcDg7JWB9TjRzRtlSUDff236NEe3zptzSwKo\n",
       "u0XvcgbqhAvy3PSoYdw25vqPtbJaMlzM+jY5Y9rySfMxXJmBuNvRt3e2ZAQzq9rZQ4i2Xc8q0wPw\n",
       "CrB5+Rt6ltBhD86y77/hugVwkIj2Xb7GLv76NS9s3QAPmo2aAAAFTQAAA+lBmiRsQS/+tSqAA3lv\n",
       "y/k0cUgetcAkK8J4mC6N35uof1Csn7mF+FikolaaPrQvNy6T0MiAybrJvpSovCcf1EOTIQ31YO/2\n",
       "2aded2FvyuPWiA0i5v6BrNr0q99GiL137V08B0UQ6YuNp7ol8biKZb3ILcZE2x+okNeT94vk9JU4\n",
       "6BV/tQyTXl5bEgOFbgKfGKVe76+DRZOInJ7gKP5Pf2VQcd+P3i7JKBmrBdPaziVvs/3inSOdV3cQ\n",
       "ia7OHOdvsrR+ODFtmg+lUhoGXM4lLwb15l/fgqhEbD5KQ+GVSo3FxAA7leX8hbSmxBd61Qm1z/6y\n",
       "tmi7ATtzuO/scHeCoHmgW1q5ryaXnKCIbaVO7FJrebQ0WdaA8H8Tt1+LVK42iY9qHsq4Gy52471f\n",
       "CmYToAmQ8Fo4PnEC9ZKXIEwcwHjPZSw9eRmExC2PY8k/jLiQvPy5OaJcjIW8SSqgTU0fb8TEIrrK\n",
       "lZHghGJswh7JAPq7kVpRSeyD7I2WfN1GYFHF4e0MG1CIHhk3UCp58mZzhs1BqnkaRiRMW+dKZwP1\n",
       "S2QMu7I/CKePEcWVo5AsTV0d2hKBg0RR/5khJywv//XEhLol8UgDq0ndFwuI44+xG27/RCTOHjrN\n",
       "e53m25IdDsndiDK/hFcSBOVQxP5OkKtDrf38Ld72d/ZN4shvIVvYZTxX1UDQgSk+GsaKCM76emMk\n",
       "2YS+X53APibTkpQlQF4vs/N7EGXBPwYrQaAQSZefJLCc9SYnf2MwzfxAXYfHsZBGrK87hMIsKJAl\n",
       "n0gKXUfF9mbxXMPv4acYOrxYlvtTHoPLUb13FneIM3UZ96sUbW+zVR7ZTHwtYn5WUpbjD+GMPC8U\n",
       "gWRvopOVEPE4sl22yMNodo0SoITvkqcWRKH/kZjUCY2bYwGc2d1ESUiWVAaTOQOT08tI/RkMFi7i\n",
       "Ov4lMGjGP3y7lk/ta9DJ92XoWz8oBI0wnYP+BqPOQNA77pfdSAixfYOCa+HDeithF22G9h14q0w2\n",
       "Av6nx4aAjJI3FVbQt+JDvyWNen69KqrHSOlUc9RMDFVnUZbBr9xvKLwusReRvOE2bBYvVrw1CFk/\n",
       "sAsw+fuA4CASVd6s2H9xxLvNmPw3+hqhkSOEVH3pSUxXLsvFcJwr+/Nd/UrIlPbR9C6sYEoEUK1s\n",
       "G6jUpKIi+iwfKYbHSdIBK3Owx1EwdqpqerLhFTJjLutVqBmS4Ikg4XtvhkDawaao+PnXRgZI3P6T\n",
       "bskag5AXWHMNiPOkpXwujrfBbh882qEwGSTNbzMw0n2LJ54i0mjXJyWF1xYlvVAjX7VwukAgyyYx\n",
       "mzeDvJBzv1Jvc0f+LETgOHhPSAAAAshBnkJ4gh8AAZuhyx4vxKAMI3QbnQrjTL56s5FiyK08JCuu\n",
       "OVQLxRx0XHqOtGHz4s40lrKAet33AG1zSDc2NSOCarNy+VEb2qyW/uk+6w2mJOw37kR2uTUEIcoE\n",
       "dItRM5cKkeqDAG5WkzNQxhco+BW5e04xhGlHwshkgM53NIk3dfwIh2Y1dQ8wt8iUFSoNc67FvuWw\n",
       "45zEV3YaSoXa1l+EYkingFPAYUf2ouwbWE3e4PnqKVeyNGhpIi6WJslYuJYHPROw5gSG7tAJA7iv\n",
       "tJjpmMt7KmshS+UD7BfjUKdqbbBi6uqg1cZZfkTK5hJ9VMMEDPTsXcgbFm8YUbujJcBN80/7GO6x\n",
       "/V1Am05QyiuCvGiVLPGSerO/Z4ERq2WbUn7OXqLmtT9FUzlyz14t3HmeLtasZa6VauH6vQ8Ho1PF\n",
       "I8a/yGlHLTwxPqoTumRSVAauTVhgbq9ge7YosNsEJIOIpic4cuYetq74M6RkYDFGbaO7+5HU2xRP\n",
       "Qnm7WoafKQxoFRrw+bFagpL2srtJn6DsZrfPB4Fp3ctYO4gvESpIx0ukKKwp5Y4b0RFtWvyPJcro\n",
       "yGawNGacd9Pr3Tvnxp/frywh4XOO6HxoIyi5kxFb+48bd3SBSoU7LUR0NKwS9EoJSlz1Kr1bE9oH\n",
       "GD/MXJhvPKf9xLhj2AZyKRF896EvpDoIQk/+QGNz4OIb0a/2fAKupwd3AEfKEKu4c6GwrqJ2qPKx\n",
       "lHZddMgx9azWJJfB7cnASN5drbhitrvrPTofz8ILnkmQGL75/Y/BiLdR74pV4GI4vsr0JWc9fQRt\n",
       "d//voGdULYJ6htrENGZoV5WPiS5IFO3mhIjF6+Yo/DPTQAvhLFfh9FRF2CBGOnDT3RI1g1GWAP8w\n",
       "3eGWy1TNYIoHSUGHr0hkOByFi+3zTOaQC2zJhCmg+q7pG8jOcnyVuaqcwP+7GVshhyZdAAAB3QGe\n",
       "YXRD/wADiV7IxeewCK7s0BCnnomNvIkZ/6mYyRV7DTOPYtp6p27wDe5zTbengtiysUYpK3tvjthA\n",
       "wcQBkrAfcBeV7L0i3dTvSh/UZN9sL38UMWAFuuj9b1hR1nsQzhD1LhxkWV4rHZCFi7rG1o7qUNYN\n",
       "y4jHoFLchup3hQJCEUGaOJctztS165a5EKBR7dYM0tHphqrsLg9x24Jlb1ylGd41htM5Ea883ndg\n",
       "6Nu7yCXkFTj2t2Q+xUhxtH75ir0y3br0/QyJzs9ATAFl9dOyYScXX747+5bTffFa4mWJXKFPzCSl\n",
       "7PjbP+iDFLDi6uFf7TX/JRB2HO9Cx5SN0tvOyReIQKBz7ilzdYi6ttNcrN9CwBfsrSsWFl4BLVpK\n",
       "+Fb4ln4FY9mnVwOx/iIEGFmrjPL+74+AvZfOW16GJ4MFD/Rjcv3993xuXqPi9dj7ZK7B+TZDPnwK\n",
       "Og9DxE6Jt3GLebVa7MMG4yjR0bsFNzSNDE/76vFpL762Z2jup2dWYJB+v7N1bb6QQViASi6uxP9e\n",
       "9bDZSLGziavixxMSouKsPItwbVzNco44g+QHnPrUBKtlCoQar0FLsanSCOEXq/Sctc4iCW6r4SkJ\n",
       "uY8IT3sf3QMN9ZzQI25wHELn0gAAAZgBnmNqQ/8AA4lbIPAIctX73r+5WJnUC/IAlrezszyrahrc\n",
       "PGya4EcnnKOFQuRGZetrqD7Hwtg6JG+nxWXjshS+cJhoxAGt2Br9AafKujRJ7zGOP+zRkNvXmRj4\n",
       "ZHPVeb+FvE1uKTfaM2MkRi+NKD2eu9x9p/W2SOzwAelgciaGlGFqjHFTNdUf6ynm2Ugas3/Z1kQj\n",
       "3SEP54avzHOEiAu3G7PWuYCNw+71KjQwmmEc+BPulxHOPCqI2aNmoleYvJrpfU/VVfDCQ40kScaO\n",
       "PHEu2/39ktxDERF+FDrud17Q6btsekYdtyvoUD0Dx6f4fuFW2LmEhMTvcaTEBLaasdSY5RqTwKlJ\n",
       "Ex4Gv3qU5vTmhCOCc4R0TOGWayN0xGmfLX+WA8lcMpOGwCFfMQQQ+fbZu3S93y/QWKp3rFUOVueR\n",
       "pid+B7eCG45Ew3IdXAhnhiU4t2xIGxF53ufsKiqBLUA9efDzIo0WB8zvmdbKsDO+yu/8ALDf7Wlm\n",
       "6gCbqMsoKuagOsuN4YOMKM5D08NsrrSuHM4vNox5D/EAAALmQZpoSahBaJlMCCH//qpVAAKUBUlO\n",
       "KGYRgJfHVmVZIR3XfXfpTWFpr+llMxxQJEKx4PTXJOUfLhqpIkRPfazZWsMR4weInehe4nq3xLT8\n",
       "jPzwn+eA27K/AEt/tKRRmpB0Vjy1R2UZqRq8ADmYWfP5QY+JeQvr6e5BS4jkOiEdfQw0n6fcOulA\n",
       "6cfj4jg7tDCqOlXziwVLbpDw4l82MFbTTftTrpVrbl6F0q8wsbvJjkmnN2UMlEqj5YyzGcJTxl6w\n",
       "v57To44U0hsbzz/VRNulk3GvTMl3ZDgCkVB034rT6Y0hrwLBKLcXLgbPR/p2HXLMpwPcNkF7FvyF\n",
       "jLyQU61i/3iE8+GaM4r0dKTu/8UqIFeHd2B973jnM9NJSn2HSIPL5dvXPc6Sm3D7JIN5zIi128Uq\n",
       "gpr4/n4yeMe05FGGl1d4uCWu7c6s3A7AUyvp+qJuLdr8MPaRf2Galb+awj1TnKT1exnGZRhxYYPB\n",
       "IhOmlILDFkHeThrhAYSwgFdhY45mfsKoyQEDOLz8k1zSNd0GLgTvD54BJRZamT4l8JjvVYxRT4nM\n",
       "ggGxGG/VYROMPpe2clY7eH/hcWiCMHIoc1g/rW5wOLfHy6Sk7ONx6DZkm+UXj9mcbEPDSRC1dhIo\n",
       "aS91cqetmTzgKaXnkBqnbyVK75ASWXOLmy5zjRcKZhiO9ihfmYvDPqF8+cR4OaF7SDnWXJdIK0nY\n",
       "unZ1syY3NpWIbos+3MlJg0D5NjFhjj2NrUfCQcIM/TybZx18RkgIPOL/15FX98W4l6/1Y7cSBQnk\n",
       "qWVveThlFZ5z+QOclOzjVl2OS3Yk/bQpPd9bxiO2YLskdqqOTlMwh81CvJ48nuvcmWgqPxjTei1a\n",
       "Zx1W8d3oW8thnIYdRnMfmeJUR5bourNEFAFYRrFDvsiuhXoQdpeGW+r1fDgXXugWgSxfNTYJGI2W\n",
       "eXkbaQg8dgei2Osxzn5RsyShgpn9/I7imGrMgF7QsWSrNQzGaQAAAilBnoZFESwQ/wABh5haBkLm\n",
       "1lFjNddjHvD/AAm3r4tkdpaMVgQi2EQyLIzpaI+K4bM6ZlHMh7ZZEm/x7I4ZMHGYX7s7UytQHZIf\n",
       "0/SRHTf8NI+p1lnT6hkIgHNjfdr8s4mfWrvNe7tqiM+pJttI0Xo1nSMLy6z/QJxYE/ZoAsoPmS89\n",
       "JFNouZctLJWJp9QP3xgLPtRwmJoFn6rJEGpEnDbCPuUQj1/ATy0xYoi8SGDSYqtFSSMmjG+JuDdf\n",
       "7+lEYwwJtnqLR/XtOLNSy3VoFn00cI8jKW+R0F8VuH44dxedOTbfv/h/4E3VKwB8zgTXXK3cvHGB\n",
       "1yhOHMaUE2pMruzFJEg4yOd2nDA2j3wuPpfTn4OVzAa9xblFBKIi4DFRTeUgm4hw0GGZAGgApj+w\n",
       "ammumcQjrvTtKKAUoX4cVptBVOunWZzjdnep7dAmH/DpcOPJND7XBcHePmKdVtneG8PaRCX49dMg\n",
       "wX4ezNm+fApUut+lFjycVfBW+onjHdOVoShVbRXG8zYSdaZUvVFlscnZzNVJZIoX7PJ3LucdmtEH\n",
       "aMYweGkn493hbTNeJn4e3rIlqThfqRjDuVI6UGDSYfriuli8iHwKkX4djQodLYGWv526qNuP+T3n\n",
       "LePXO0Q7KPsGf1LaEE4hKUvRGT1Ft2kWrgD2KDZwJUL8cN29nWZhEYAv4cSN+RgEIXfaLYcGj3wS\n",
       "nZeJwTVz4DF4VlDwQiVB0nlOviz63kDdAAABRQGepXRD/wADX3SN3ngL/na8995tMcl3d3TuFKcw\n",
       "qzGCya7CNkCkloshJ/JNhmat3oHtLTs2r25TL2ffQG//uQf1SH4YeU9WodwRJjTf9uPEEp9lttY0\n",
       "9keb026By+3iueE5pwbQ4lRRdaN+cy0E2f5QYFNhNkO20hmCWRG5HSv7G7AZfV8LVoajTDk8qkAC\n",
       "3ytGyz0PfZyy0Sihe1AAHsqsGecviJjQjQuX0o+YUOlBhxl37ubi0C47NlkZvdXjv7dzICh+8zQs\n",
       "fwXkguiHPEsDb2oZpn1S4XrCm1R15PLvETq7rpH/4c+2rFwbqzccqiq8b6dF/kZb5dWcvXYRqxN/\n",
       "qIZC2zwIiSjb65sr8PAJzLSvQ3prDlP82IRt35Fv+eaIEOhHbYP6eT81Yvx00xDXWZdKHSZ3gtI5\n",
       "vRKVeIxG/pw45L0AAAF5AZ6nakP/AANLhuTEaWHC3Ihtnqz3AGF9icXFecw69iglqJfUfzyEQ/yj\n",
       "9tDuE+StilltnUNFMzLzEXfb1dzfgNgpzh2MLNcdDTIYz4JWez5OiNc7MYxBjj/gFG8LmxtzdvsP\n",
       "BnL+9evVeJ1N9hOU4DHH82g+UhgJt26qNpdLt68rAzDgFLAbYuQJkfuXo6hna+39pciCMKxGISWN\n",
       "+uOBCNfAdUdbnz4qymaIeqhZ2ubNVrw5RvTXFt1tGVws5bYPm4A3qfywwwqn+MusWtLWDlEs9xmm\n",
       "Iyub6j9pP+mZuIpvUvFWpYgCZvUUfMtr/Wl2v9eFS41oGqIwMlnScsK6M1IMZIvyaPHdTtQUwXQx\n",
       "ETtS1if3BjRqVYhIKEbYLHvhbujbBF/TBFNAEW5pbSg2vmhYtKZafdpqNJNvJyKoxhspI4H+gCuS\n",
       "3vFAquAK4PFbf2Xqcrj38VrCnez4JgtEYe+FfuYgB7G0OD0Z/2W9l8o0RtfDKW2Ro9wAAAG3QZqp\n",
       "SahBbJlMCH///qmWAAmBOq4qINL6AWruW/r5sFLnnOus6hx3UOLQ/cGKn/shOSiPVz2JCnndbmg2\n",
       "Y+lab6MVW5JVdDRa4YyqNVhRM2k9l87yWh9Ejibss4MmnWPQ/MJh0M8IEqy2O99AK9zw6LkYvg6g\n",
       "n5eDx/VaodJV+p7cadNakQV2/aQgBdzvMLuG72klpeaf0fkWPwPV5bcJBmRG1AC5YQlbVi5VahBj\n",
       "hWhGzde6eoAXfrvg7yCpG42txHzkc/7xymRuS6p6nAze6t1qwAYl0G69HhhsUqtJx5BeYeL6AR2P\n",
       "hSvr/UmIDgTQm5Bf3My7/XO+NLY0kgDJOluNVeGQB1GnVUr7IpI8ieLs4xGvs4o3QbVV7NumwT9b\n",
       "+B3BPVTbB/nOJuG95K0ni1BBh6AqwYd/mXbPmZImuivuEfPlQSiqP1QzCLPIyBvi/DwfvkhCAvlC\n",
       "euiXdPYQ0cGVwrM4A3cjrbrPHS0/FEx0rEXYgUDoyZlAoQBlbbf3wX7B9xPkTXs7IcaoA9cFQsJu\n",
       "z5mcjD2k0CmqBC9/NRbBHCtTu/RvchW6Chg6kg5Lud1C9N1YMAAAA6Ztb292AAAAbG12aGQAAAAA\n",
       "AAAAAAAAAAAAAAPoAAAH0AABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAA\n",
       "AAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAC0HRyYWsAAABcdGtoZAAA\n",
       "AAMAAAAAAAAAAAAAAAEAAAAAAAAH0AAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEA\n",
       "AAAAAAAAAAAAAAAAAEAAAAACgAAAAeAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAB9AAABAA\n",
       "AAEAAAAAAkhtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAACgAAABQAFXEAAAAAAAtaGRscgAAAAAA\n",
       "AAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAHzbWluZgAAABR2bWhkAAAAAQAAAAAA\n",
       "AAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAABs3N0YmwAAACzc3RzZAAA\n",
       "AAAAAAABAAAAo2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAACgAHgAEgAAABIAAAAAAAAAAEA\n",
       "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAxYXZjQwFkABb/4QAYZ2QAFqzZ\n",
       "QKA9oQAAAwABAAADAAoPFi2WAQAGaOvjyyLAAAAAHHV1aWRraEDyXyRPxbo5pRvPAyPzAAAAAAAA\n",
       "ABhzdHRzAAAAAAAAAAEAAAAKAAAIAAAAABRzdHNzAAAAAAAAAAEAAAABAAAAYGN0dHMAAAAAAAAA\n",
       "CgAAAAEAABAAAAAAAQAAKAAAAAABAAAQAAAAAAEAAAAAAAAAAQAACAAAAAABAAAoAAAAAAEAABAA\n",
       "AAAAAQAAAAAAAAABAAAIAAAAAAEAABAAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAAAKAAAAAQAAADxz\n",
       "dHN6AAAAAAAAAAAAAAAKAAAVzgAAA+0AAALMAAAB4QAAAZwAAALqAAACLQAAAUkAAAF9AAABuwAA\n",
       "ABRzdGNvAAAAAAAAAAEAAAAsAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRp\n",
       "cmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1OC4yOS4x\n",
       "MDA=\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_vid = vids.data[3]['trace_videos'][45]\n",
    "anim = animate_frames(rand_vid)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset -  to do find the buggest boxes and add padding to everything\n",
    "box_shape = (100, 100) #TO DO: find the biggest box and set it to this\n",
    "T = 10\n",
    "\n",
    "class CellBoxMaskPatch(torch.utils.data.Dataset):\n",
    "    #input will be a Directory name, function is TO DO\n",
    "    def __init__(self, vid_dir, T=T):\n",
    "        \n",
    "        self.T = T\n",
    "        \n",
    "\n",
    "        # list of dictionaries, each dictionary is one cell tracked throughout a video\n",
    "        #each cell has 3 data types in a dictionary\n",
    "            #mask, box and patches\n",
    "        self.cell_dict = self.get_dicts(vid_dir)\n",
    "\n",
    "        #our \"end tokens\"\n",
    "        self.end_box = (-1,-1,-1,-1)\n",
    "        self.end_masks = np.zeros([box_shape[0], box_shape[1]]).flatten()\n",
    "        self.end_patches = np.zeros([box_shape[0], box_shape[1]]).flatten()\n",
    "        \n",
    "        # How many total cells in dataset\n",
    "        self.num_cells = len(self.cell_dict)\n",
    "        self.num_timesteps, self.list_timesteps_start, self.list_timesteps_end = self.get_num_timesteps()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_timesteps\n",
    "\n",
    "    def get_dicts(self, vid_dir):\n",
    "        #returns a of list of dictionaries for every cell\n",
    "\n",
    "        return fake_data\n",
    "    \n",
    "    def get_num_timesteps(self):\n",
    "        #count the number of items in each list of each cell\n",
    "        count= 0\n",
    "        list_counts_start = [0]\n",
    "        list_counts_end = []\n",
    "        for cell in self.cell_dict:\n",
    "            current_count = len(cell['box'])\n",
    "            count+=current_count\n",
    "            list_counts_end.append(count)\n",
    "            list_counts_start.append(count)\n",
    "        return count, list_counts_start[:-1], list_counts_end#list counts returns the start of every cell\n",
    "\n",
    "    def binary_search(self, box_end, box_starts, number):\n",
    "        #performs a binary search on the cells, to find the right timestep and cell number for a given index\n",
    "        #if this gets slow, can edit to create a dictionary in get_num_timesteps\n",
    "        left, right = 0, len(box_starts) - 1\n",
    "        while left <= right:\n",
    "            mid = left + (right - left) // 2\n",
    "            if box_starts[mid] <= number < box_end[mid]:\n",
    "                # Found the box\n",
    "                box_number = mid  \n",
    "                position_in_box = number - box_starts[mid]\n",
    "                # print (number,  \" found in box \", box_number, \", \" , position_in_box)\n",
    "                return box_number, position_in_box\n",
    "            elif number < box_starts[mid]:\n",
    "                right = mid - 1\n",
    "            else:\n",
    "                left = mid + 1\n",
    "    \n",
    "        # Number not found in any box\n",
    "        raise ValueError(f\"Number {number} is not found in any cell and timestep combination. Length shoudl be at most\", self.num_timesteps)\n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx, type='image'):\n",
    "        #should return a dictionary for the cell, and T dictionaries for the next T cells. \n",
    "        #If any cells don't exist (ie we look for index n, n+1, n+2, n+3..) and the list is of \n",
    "        #length n+2, everything that doesn't exist (n+2, n+3...) should just be self.end, our \"end token\"\n",
    "    \n",
    "        #turn start and cell\n",
    "        # print(self.list_timesteps_start, self.list_timesteps_end)\n",
    "\n",
    "        cell_idx, start = self.binary_search(self.list_timesteps_end, self.list_timesteps_start, idx)\n",
    "\n",
    "        cell = self.cell_dict[cell_idx]\n",
    "\n",
    "        boxes = cell[\"box\"]\n",
    "        #flatten\n",
    "        masks = cell[\"masks\"]\n",
    "        #flatten\n",
    "        patches = cell[\"patch\"]\n",
    "\n",
    "        box = boxes[start]\n",
    "        mask = masks[start]\n",
    "        patch = patches[start]\n",
    "\n",
    "        # Get the dictionaries for the next T cells\n",
    "        next_T_frames_box = []\n",
    "        next_T_frames_masks = []\n",
    "        next_T_frames_patches = []\n",
    "        for i in range(1, self.T + 1):\n",
    "            if start + i < self.list_timesteps_end[cell_idx]-self.list_timesteps_start[cell_idx]:\n",
    "                next_T_frames_box.append(boxes[start + i])\n",
    "                next_T_frames_masks.append(masks[start+1].flatten())\n",
    "                next_T_frames_patches.append(patches[start+i].flatten())\n",
    "            else:\n",
    "                # Append the default value `self.end` if the cell doesn't exist\n",
    "                next_T_frames_box.append(self.end_box)\n",
    "                next_T_frames_masks.append(self.end_masks)\n",
    "                next_T_frames_patches.append(self.end_patches)\n",
    "\n",
    "        #returns tuples, list of tuples\n",
    "        return (box, mask, patch), (next_T_frames_box, next_T_frames_masks, next_T_frames_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "cell_folder = \"folder_with_cells_or_dicts_not_sure_implementation_rn\"\n",
    "\n",
    "dataset = CellBoxMaskPatch(cell_folder, 1) #looking only one timestep ahead\n",
    "\n",
    "train, eval, test = random_split(dataset, [0.4, 0.2, 0.4])\n",
    "\n",
    "input_datasets = {}\n",
    "input_datasets[\"train\"] = train\n",
    "input_datasets[\"eval\"] = eval\n",
    "input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep = [[0] for t in range(T)]\n",
    "# timestep\n",
    "\n",
    "# batch = ((np.array([1 , 2, 3]), 1, 2) , 3), ((np.array([1 , 2, 3]), 1, 2) , 3)\n",
    "# current_boxes = torch.stack([torch.tensor(b[0][0], dtype=torch.long) for b in batch], dim=0)\n",
    "# current_boxes\n",
    "        #current     #next   #boxes                     #masks\n",
    "batch  = [[1, 2, 3], [[[1,2, 3, 4], [1, 2, 3,4]], [[1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1,2, 3]]]], [[1, 2, 3], [[[1,2, 3, 4], [1, 2, 3,4]], [[1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1,2, 3]]]]\n",
    "\n",
    "\n",
    "\n",
    "next_boxes = torch.stack([torch.tensor(b[1][0], dtype=torch.long) for b in batch], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, mode_box, mode_mask, mode_patch, T):\n",
    "    #this should stick the batches together in way that makes sense in a torch stack manner\n",
    "    #we want to return dim [B, 1, d], [B, T, d] where d is a combination of 4 (for boxes) and the flattened image shape\n",
    "    #taking only taking one cell from sequence before and T is sequence after\n",
    "    #if mode_box is true, we want to return [B, 1, 4] and [B, T, 4]\n",
    "    #if mode_mask is true, we want to return [B, 1, (10,10).flatten] and [B, T, (10,10).flatten]\n",
    "    #input (cur_box, cur_mask, cur_patch), (T_box, T_mask, T_patch)\n",
    "  \n",
    "    current_boxes = torch.stack([torch.tensor(b[0][0], dtype=torch.long) for b in batch], dim=0)\n",
    "    current_masks = torch.stack([torch.tensor(b[0][1], dtype=torch.long) for b in batch], dim=0)\n",
    "    current_patches = torch.stack([torch.tensor(b[0][2], dtype=torch.long) for b in batch], dim=0)\n",
    "    selected_tensors = []\n",
    "\n",
    "\n",
    "    next_boxes = torch.stack([torch.tensor(b[1][0], dtype=torch.int) for b in batch], dim=0)\n",
    "    next_masks = torch.stack([torch.tensor(b[1][1], dtype=torch.float64) for b in batch], dim=0)\n",
    "    # next_masks = torch.stack([torch.tensor(b[1][1], dtype=torch.int) for b in batch], dim=0)\n",
    "    next_patches = torch.stack([torch.tensor(b[1][2], dtype=torch.float64) for b in batch], dim=0)\n",
    "    selected_next_tensors = []\n",
    "    print(\"selected\")\n",
    "\n",
    "    if mode_box:\n",
    "        selected_tensors.append(current_boxes)\n",
    "        selected_next_tensors.append(next_boxes)\n",
    "    if mode_mask:\n",
    "        selected_tensors.append(current_masks)\n",
    "        selected_next_tensors.append(next_masks)\n",
    "    if mode_patch:\n",
    "        selected_tensors.append(current_patches)\n",
    "        selected_next_tensors.append(next_patches)\n",
    "\n",
    "\n",
    "    # Concatenate selected tensors along the last dimension\n",
    "    combined_tensor = torch.cat(selected_tensors, dim=-1)\n",
    "    combined_next_tensors = torch.cat(selected_next_tensors, dim=-1)\n",
    "\n",
    "    # Reshape to add the singleton dimension\n",
    "    combined_tensor = combined_tensor.unsqueeze(1)\n",
    "\n",
    "    # Verify the shape\n",
    "    print(combined_tensor.shape, combined_next_tensors.shape)\n",
    "\n",
    "\n",
    "    return combined_tensor, combined_next_tensors\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_box = True\n",
    "mode_mask = False\n",
    "mode_patch = False\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch, T)\n",
    ")\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['test'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch, T)\n",
    ")\n",
    "\n",
    "dataloaders['eval'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['eval'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch, T)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected\n",
      "torch.Size([4, 1, 4]) torch.Size([4, 1, 4])\n",
      "Input: (tensor([[[1, 2, 3, 5]],\n",
      "\n",
      "        [[1, 2, 3, 4]],\n",
      "\n",
      "        [[1, 2, 3, 5]],\n",
      "\n",
      "        [[1, 2, 3, 4]]]), tensor([[[-1, -1, -1, -1]],\n",
      "\n",
      "        [[ 1,  2,  3,  5]],\n",
      "\n",
      "        [[-1, -1, -1, -1]],\n",
      "\n",
      "        [[ 1,  2,  3,  5]]], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders['train']:\n",
    "    # input, output = batch\n",
    "    # Process the batches as needed\n",
    "    # print(\"Input Shape:\", input.shape)\n",
    "    # print(\"Output Shape:\", output.shape)\n",
    "    print(\"Input:\", batch)\n",
    "    \n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for mask and patch\n",
    "input_dim =  box_shape #TO DO: find the biggest box and set it to this\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "T = 2 #len predictions\n",
    "hidden_dims = 100\n",
    "output_dim = input_dim\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ 1+ np.exp(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an LSTM RNN\n",
    "\n",
    "#to create a pytorch LSTM \n",
    "#The first axis is the sequence itself\n",
    "# the second indexes instances in the mini-batch\n",
    "# third indexes elements of the input. \n",
    "\n",
    "# we would input T boxes\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        #input_size, hidden_size, num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2) #stacking 2 LSTMs\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lstm_out, h_n = self.lstm(input, batch_size, -1) #output and final hidden state\n",
    "        output = self.hidden2out(lstm_out.view(len(input), -1))\n",
    "        #output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 1, 2])\n",
      "Output shape: torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, batch_first=True):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=batch_first)\n",
    "        self.hidden2out = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, T):\n",
    "        # x has shape [B, 1, d]\n",
    "        # Repeat the input tensor along the time dimension\n",
    "        x = x.repeat(1, T, 1)\n",
    "        # Pass through LSTM\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.hidden2out(output)\n",
    "        return output\n",
    "\n",
    "# Example input data\n",
    "batch_size = 3\n",
    "seq_length = 4\n",
    "input_dim = 2\n",
    "hidden_size = 5\n",
    "num_layers = 1\n",
    "input_data = torch.randn(batch_size, 1, input_dim)\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = MyLSTM(input_dim, hidden_size, num_layers)\n",
    "\n",
    "# Forward pass with desired sequence length T\n",
    "T = seq_length\n",
    "output = lstm_model(input_data, T)\n",
    "\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'box': [(1, 2, 3, 4), (1, 2, 3, 5)], 'masks': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]]), 'patch': array([[1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       ...,\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.],\n",
      "       [1., 1., 1., ..., 1., 1., 1.]])}\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "for _ in range(50):\n",
    "    # Create a dictionary for each item\n",
    "    item_dict = {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],  # Example box values\n",
    "        \"masks\": np.ones((100, 100)),  # Example masks array\n",
    "        \"patch\": np.ones((100, 100))   # Example patch array\n",
    "    }\n",
    "    # Append the dictionary to the list\n",
    "    data_list.append(item_dict)\n",
    "\n",
    "# Print the first item to verify the structure\n",
    "print(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dim = 0\n",
    "if mode_box == True:\n",
    "    input_dim +=  4 #(T, 4) for the bounding boxes\n",
    "if mode_mask == True:\n",
    "    input_dim += box_shape.flatten #flattened version of [H, W]\n",
    "if mode_patch == True:\n",
    "    input_dim += box_shape.flatten #flattened version of [H, W]\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "T = 2\n",
    "hidden_dims = 100\n",
    "output_dim = input_dim\n",
    "batch_size = 1\n",
    "model = LSTM(input_dim, hidden_dims, output_dim)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your training loop + eval, etc...\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'dev']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]: #should be a [B, T, S] batch, time, sequence shape\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                 # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, labels) #B, T, [size of input]\n",
    "                    _, preds = torch.max(outputs, dim=1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'dev' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
