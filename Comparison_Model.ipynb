{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f004ebe3700>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aicspylibczi import CziFile\n",
    "import czifile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "import ffmpeg\n",
    "import time\n",
    "import pandas as pd\n",
    "from cellpose import io, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoDataMIP:\n",
    "    def __init__(self, files):\n",
    "        self.data = {\n",
    "        }\n",
    "        \n",
    "        for category, num in files:\n",
    "            print(f\"Loading in MIP {num}\")\n",
    "            assert category == 'mip', \"Can't load non Mip file\"\n",
    "            file = {}\n",
    "            file['video'] = get_file(category, num)\n",
    "            \n",
    "            frames, shp = file['video'].read_image(C=0)\n",
    "            frames = scale_img(frames.squeeze())\n",
    "            file['frames'] = frames\n",
    "            print(f\"frames {num}: {frames.shape}\")\n",
    "            file['masks'] = binarize_video(frames)           \n",
    "    \n",
    "            self.data[num] = file    \n",
    "    def extract_all_traces(self, file_num, sequence_length, hist_length=2):\n",
    "        # hist length is how many frames of history\n",
    "        frames, masks = self.data[file_num]['frames'], self.data[file_num]['masks']\n",
    "        N = len(frames)\n",
    "        s = 0\n",
    "        all_traces = []\n",
    "        all_videos = []\n",
    "        for i in range(N // sequence_length):\n",
    "            print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "            data, videos = extract_traces(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "            s += sequence_length\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos\n",
    "        \n",
    "        if(N % sequence_length > 0):\n",
    "            data, videos = extract_traces(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos        \n",
    "        self.data[file_num]['traces'] = all_traces\n",
    "        self.data[file_num]['trace_videos'] = all_videos\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = [\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones([100, 100])],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones([100, 100])]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    },\n",
    "    {\n",
    "        \"box\": [(1, 2, 3, 4), (1, 2, 3, 5)],\n",
    "        \"masks\": [np.ones([100, 100]), np.ones((100, 100))],\n",
    "        \"patch\": [np.ones([100, 100]), np.ones((100, 100))]\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "fake_data[0][\"masks\"][1].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import centroid\n",
    "\n",
    "\n",
    "box_shape = (100, 100) #TO DO: find the biggest box and set it to this\n",
    "X = 10\n",
    "\n",
    "class CellBoxMaskPatch(torch.utils.data.Dataset):\n",
    "    #input will be a Directory name, function is TO DO\n",
    "    def __init__(\n",
    "        self,\n",
    "        files, \n",
    "        X=X):\n",
    "        \n",
    "        self.video_extractor = VideoDataMIP(files)\n",
    "\n",
    "        # list of dictionaries, each dictionary is one cell tracked throughout a video\n",
    "        #each cell has 3 data types in a dictionary\n",
    "            #mask, box and patches\n",
    "\n",
    "        for i in files:\n",
    "            self.video_extractor.extract_all_traces(i[1], X)\n",
    "        \n",
    "        \n",
    "        self.cell_dict = []\n",
    "\n",
    "        for key in self.video_extractor.data:\n",
    "            entry = self.video_extractor.data[key][\"traces\"]\n",
    "            for cell in entry:\n",
    "                print(cell.keys())\n",
    "                self.cell_dict.append({\n",
    "                    \"boxes\": cell[\"boxes\"],\n",
    "                    \"patches\": cell[\"patches\"],\n",
    "                    \"masks\": cell[\"masks\"]\n",
    "                })\n",
    "            \n",
    "\n",
    "        self.num_cells = len(self.cell_dict)\n",
    "        # print(\"the number is cells is\", self.num_cells)\n",
    "        # first_cell = self.cell_dict[0]\n",
    "        # print(\"this is the keys of the first cell\", first_cell.keys())\n",
    "        # print(\"this is the amount of boxes in this cell\", len(first_cell['boxes']))\n",
    "\n",
    "\n",
    "        # self.num_timesteps, self.list_timesteps_start, self.list_timesteps_end = self.get_num_timesteps()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_timesteps\n",
    "\n",
    "    # def get_num_timesteps(self):\n",
    "    #     #count the number of items in each list of each cell\n",
    "    #     count= 0\n",
    "    #     list_counts_start = [0]\n",
    "    #     list_counts_end = []\n",
    "    #     for cell in self.cell_dict:\n",
    "    #         current_count = len(cell['boxes'])\n",
    "    #         count+=current_count\n",
    "    #         list_counts_end.append(count)\n",
    "    #         list_counts_start.append(count)\n",
    "    #     print(\"there are \", count, \" timesteps of \", self.num_cells, \" cells \", \"this is what index they all start at \\n\", list_counts_start, \"\\n and end \\n\", list_counts_end)\n",
    "    #     return count, list_counts_start[:-1], list_counts_end#list counts returns the start of every cell\n",
    "\n",
    "    # def binary_search(self, box_end, box_starts, number):\n",
    "    #     #performs a binary search on the cells, to find the right timestep and cell number for a given index\n",
    "    #     #if this gets slow, can edit to create a dictionary in get_num_timesteps\n",
    "    #     left, right = 0, len(box_starts) - 1\n",
    "    #     while left <= right:\n",
    "    #         mid = left + (right - left) // 2\n",
    "    #         if box_starts[mid] <= number < box_end[mid]:\n",
    "    #             # Found the box\n",
    "    #             box_number = mid  \n",
    "    #             position_in_box = number - box_starts[mid]\n",
    "    #             # print (number,  \" found in box \", box_number, \", \" , position_in_bsox)\n",
    "    #             return box_number, position_in_box\n",
    "    #         elif number < box_starts[mid]:\n",
    "    #             right = mid - 1\n",
    "    #         else:\n",
    "    #             left = mid + 1\n",
    "    \n",
    "    #     # Number not found in any box\n",
    "    #     raise ValueError(f\"Number {number} is not found in any cell and timestep combination. Length shoudl be at most\", self.num_timesteps)\n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #should T dictiaries for . \n",
    "        #If any cells don't exist (ie we look for index n, n+1, n+2, n+3..) and the list is of \n",
    "        #length n+2, everything that doesn't exist (n+2, n+3...) should just be self.end, our \"end token\"\n",
    "    \n",
    "        #turn start and cell\n",
    "        # print(self.list_timesteps_start, self.list_timesteps_end)\n",
    "\n",
    "        # cell_idx, start = self.binary_search(self.list_timesteps_end, self.list_timesteps_start, idx)\n",
    "\n",
    "        cell = self.cell_dict[idx]\n",
    "\n",
    "        boxes = cell[\"boxes\"]\n",
    "        masks = cell[\"masks\"]\n",
    "        patches = cell[\"patch\"]\n",
    "\n",
    "\n",
    "        #make the masks all 1's for all the cells\n",
    "        for cell_mask in np.arange(masks): #there are 67 cells, each cell is a list of 10 cells\n",
    "            for cell_time in masks[cell_mask]:\n",
    "                cell_time = np.array(cell_time, dtype=np.int32)\n",
    "                cell_time = np.where(cell_time < 0, cell_time, 1)\n",
    "                masks[cell_mask] = cell_time\n",
    "\n",
    "        # box = boxes[start]\n",
    "        # mask = masks[start]\n",
    "        # patch = patches[start]\n",
    "\n",
    "        # # Get the dictionaries for the next T cells\n",
    "        # next_T_frames_box = []\n",
    "        # next_T_frames_masks = []\n",
    "        # next_T_frames_patches = []\n",
    "        # for i in range(1, self.T + 1):\n",
    "        #     # print(\"Getting next frame\", i)\n",
    "        #     if start + i < self.list_timesteps_end[cell_idx]-self.list_timesteps_start[cell_idx]:\n",
    "        #         next_T_frames_box.append(boxes[start + i])\n",
    "        #         next_T_frames_masks.append(masks[start+1].flatten())\n",
    "        #         #change all the numbers to smth else --> do do mask\n",
    "        #         next_T_frames_patches.append(patches[start+i].flatten())\n",
    "            \n",
    "        \n",
    "        # print(\"next T frames shape\", np.array(next_T_frames_box).shape)\n",
    "        #returns tuples, list of tuples\n",
    "                # from [1, d1, d2, d3, 10] becomes [B, 10, d]. #centroids \n",
    "\n",
    "        centroids = ([centroid(t) for t in patches])\n",
    "\n",
    "        return (boxes, masks, patches),                         centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in MIP 3\n",
      "Loading dicty_factin_pip3-03_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 90)}]\n",
      "frames 3: (90, 2048, 474)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "root_dir = \"/mnt/datadisk/\"\n",
    "subdirs = ['FactinProcessed', 'FactinMIP']\n",
    "\n",
    "mip_video_files = [\n",
    "    ('mip', 3),\n",
    "    ('mip', 6),\n",
    "    ('mip', 9),\n",
    "]\n",
    "\n",
    "mip_test_files = [('mip', 3)] #so that I dont keep crashing the kernel\n",
    "\n",
    "# file = root_dir + subdirs\n",
    "\n",
    "video_extractor = VideoDataMIP(mip_test_files)\n",
    "\n",
    "        # list of dictionaries, each dictionary is one cell tracked throughout a video\n",
    "        #each cell has 3 data types in a dictionary\n",
    "            #mask, box and patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting traces from 0:10\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting traces from 10:20\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 20:30\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 30:40\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 40:50\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 50:60\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 60:70\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting traces from 70:80\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting traces from 80:90\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n"
     ]
    }
   ],
   "source": [
    "X=10\n",
    "\n",
    "\n",
    "for i in ([3]):\n",
    "    video_extractor.extract_all_traces(i, X)\n",
    "\n",
    "cell_dict_with_patches = video_extractor.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 56)\n",
      "(102, 57)\n",
      "(83, 55)\n",
      "(151, 118)\n",
      "(66, 46)\n",
      "(62, 58)\n",
      "(40, 35)\n",
      "(24, 25)\n",
      "(119, 69)\n",
      "(118, 68)\n",
      "(95, 111)\n",
      "(86, 95)\n",
      "(99, 90)\n",
      "(151, 118)\n",
      "(66, 46)\n",
      "(62, 58)\n",
      "(40, 35)\n",
      "(24, 25)\n",
      "(119, 69)\n",
      "(118, 68)\n",
      "(142, 104)\n",
      "(144, 105)\n",
      "(143, 104)\n",
      "(145, 106)\n",
      "(138, 108)\n",
      "(140, 109)\n",
      "(144, 117)\n",
      "(152, 117)\n",
      "(152, 118)\n",
      "(156, 118)\n",
      "(34, 21)\n",
      "(35, 23)\n",
      "(35, 23)\n",
      "(39, 24)\n",
      "(40, 26)\n",
      "(36, 23)\n",
      "(34, 23)\n",
      "(39, 25)\n",
      "(35, 25)\n",
      "(40, 25)\n",
      "(84, 90)\n",
      "(83, 67)\n",
      "(82, 83)\n",
      "(78, 79)\n",
      "(81, 69)\n",
      "(88, 67)\n",
      "(85, 65)\n",
      "(94, 63)\n",
      "(92, 65)\n",
      "(97, 65)\n",
      "(94, 85)\n",
      "(96, 76)\n",
      "(105, 87)\n",
      "(109, 79)\n",
      "(124, 53)\n",
      "(141, 55)\n",
      "(133, 57)\n",
      "(126, 79)\n",
      "(105, 81)\n",
      "(101, 87)\n",
      "(100, 71)\n",
      "(96, 73)\n",
      "(96, 74)\n",
      "(114, 75)\n",
      "(100, 84)\n",
      "(93, 83)\n",
      "(94, 88)\n",
      "(82, 89)\n",
      "(79, 90)\n",
      "(82, 76)\n",
      "(138, 73)\n",
      "(147, 71)\n",
      "(134, 75)\n",
      "(144, 70)\n",
      "(145, 78)\n",
      "(133, 89)\n",
      "(119, 104)\n",
      "(131, 121)\n",
      "(134, 122)\n",
      "(91, 135)\n",
      "(56, 28)\n",
      "(147, 71)\n",
      "(134, 75)\n",
      "(144, 70)\n",
      "(145, 78)\n",
      "(133, 89)\n",
      "(119, 104)\n",
      "(131, 121)\n",
      "(134, 122)\n",
      "(91, 135)\n",
      "(153, 116)\n",
      "(148, 119)\n",
      "(143, 117)\n",
      "(136, 116)\n",
      "(146, 122)\n",
      "(147, 117)\n",
      "(147, 115)\n",
      "(136, 111)\n",
      "(135, 114)\n",
      "(126, 114)\n",
      "(39, 24)\n",
      "(39, 27)\n",
      "(40, 25)\n",
      "(36, 24)\n",
      "(34, 24)\n",
      "(39, 25)\n",
      "(34, 23)\n",
      "(40, 25)\n",
      "(39, 26)\n",
      "(35, 24)\n",
      "(90, 66)\n",
      "(92, 66)\n",
      "(88, 64)\n",
      "(93, 66)\n",
      "(88, 68)\n",
      "(90, 70)\n",
      "(84, 78)\n",
      "(87, 83)\n",
      "(91, 79)\n",
      "(93, 69)\n",
      "(101, 91)\n",
      "(85, 94)\n",
      "(90, 95)\n",
      "(87, 78)\n",
      "(86, 86)\n",
      "(82, 101)\n",
      "(77, 99)\n",
      "(95, 89)\n",
      "(129, 76)\n",
      "(149, 68)\n",
      "(81, 76)\n",
      "(130, 96)\n",
      "(117, 102)\n",
      "(123, 113)\n",
      "(128, 117)\n",
      "(140, 107)\n",
      "(158, 107)\n",
      "(111, 110)\n",
      "(109, 100)\n",
      "(99, 103)\n",
      "(57, 112)\n",
      "(43, 114)\n",
      "(44, 105)\n",
      "(45, 110)\n",
      "(40, 122)\n",
      "(32, 132)\n",
      "(68, 123)\n",
      "(15, 69)\n",
      "(9, 66)\n",
      "(23, 34)\n",
      "(75, 140)\n",
      "(74, 142)\n",
      "(69, 154)\n",
      "(94, 128)\n",
      "(123, 119)\n",
      "(126, 96)\n",
      "(113, 95)\n",
      "(113, 110)\n",
      "(122, 113)\n",
      "(108, 107)\n",
      "(87, 98)\n",
      "(102, 90)\n",
      "(110, 83)\n",
      "(104, 73)\n",
      "(116, 75)\n",
      "(131, 79)\n",
      "(139, 87)\n",
      "(137, 49)\n",
      "(131, 64)\n",
      "(130, 67)\n",
      "(128, 111)\n",
      "(124, 113)\n",
      "(122, 117)\n",
      "(124, 113)\n",
      "(116, 112)\n",
      "(115, 114)\n",
      "(115, 109)\n",
      "(116, 109)\n",
      "(121, 109)\n",
      "(121, 107)\n",
      "(28, 23)\n",
      "(34, 23)\n",
      "(28, 23)\n",
      "(27, 22)\n",
      "(34, 23)\n",
      "(26, 24)\n",
      "(34, 23)\n",
      "(34, 23)\n",
      "(36, 23)\n",
      "(29, 23)\n",
      "(93, 66)\n",
      "(99, 66)\n",
      "(100, 66)\n",
      "(100, 65)\n",
      "(96, 64)\n",
      "(102, 64)\n",
      "(95, 65)\n",
      "(87, 65)\n",
      "(86, 66)\n",
      "(85, 65)\n",
      "(130, 60)\n",
      "(137, 70)\n",
      "(146, 64)\n",
      "(134, 68)\n",
      "(108, 74)\n",
      "(111, 84)\n",
      "(118, 75)\n",
      "(120, 59)\n",
      "(122, 59)\n",
      "(134, 58)\n",
      "(100, 106)\n",
      "(93, 103)\n",
      "(88, 82)\n",
      "(84, 82)\n",
      "(82, 91)\n",
      "(81, 81)\n",
      "(78, 78)\n",
      "(70, 78)\n",
      "(70, 78)\n",
      "(66, 78)\n",
      "(100, 72)\n",
      "(93, 70)\n",
      "(113, 77)\n",
      "(133, 64)\n",
      "(153, 53)\n",
      "(170, 49)\n",
      "(159, 49)\n",
      "(158, 65)\n",
      "(132, 57)\n",
      "(127, 66)\n",
      "(23, 46)\n",
      "(33, 51)\n",
      "(23, 55)\n",
      "(23, 44)\n",
      "(19, 24)\n",
      "(23, 19)\n",
      "(159, 49)\n",
      "(158, 65)\n",
      "(132, 57)\n",
      "(127, 66)\n",
      "(112, 108)\n",
      "(102, 113)\n",
      "(122, 98)\n",
      "(124, 91)\n",
      "(121, 87)\n",
      "(124, 78)\n",
      "(117, 68)\n",
      "(116, 63)\n",
      "(121, 65)\n",
      "(103, 66)\n",
      "(127, 70)\n",
      "(119, 79)\n",
      "(133, 85)\n",
      "(131, 75)\n",
      "(122, 84)\n",
      "(111, 83)\n",
      "(105, 78)\n",
      "(118, 84)\n",
      "(141, 73)\n",
      "(146, 72)\n",
      "(116, 109)\n",
      "(118, 108)\n",
      "(119, 110)\n",
      "(117, 113)\n",
      "(118, 110)\n",
      "(117, 108)\n",
      "(115, 108)\n",
      "(115, 110)\n",
      "(115, 111)\n",
      "(117, 112)\n",
      "(28, 23)\n",
      "(33, 24)\n",
      "(28, 23)\n",
      "(33, 25)\n",
      "(27, 23)\n",
      "(28, 23)\n",
      "(33, 24)\n",
      "(32, 24)\n",
      "(29, 23)\n",
      "(29, 23)\n",
      "(84, 67)\n",
      "(80, 66)\n",
      "(74, 69)\n",
      "(88, 68)\n",
      "(93, 66)\n",
      "(98, 66)\n",
      "(98, 66)\n",
      "(92, 66)\n",
      "(84, 69)\n",
      "(75, 68)\n",
      "(110, 65)\n",
      "(107, 80)\n",
      "(98, 75)\n",
      "(112, 76)\n",
      "(127, 73)\n",
      "(125, 74)\n",
      "(99, 78)\n",
      "(91, 79)\n",
      "(108, 82)\n",
      "(112, 77)\n",
      "(123, 66)\n",
      "(99, 75)\n",
      "(75, 81)\n",
      "(62, 82)\n",
      "(54, 90)\n",
      "(63, 90)\n",
      "(62, 62)\n",
      "(53, 41)\n",
      "(39, 42)\n",
      "(25, 26)\n",
      "(62, 77)\n",
      "(66, 79)\n",
      "(65, 77)\n",
      "(58, 65)\n",
      "(56, 64)\n",
      "(53, 56)\n",
      "(51, 53)\n",
      "(46, 44)\n",
      "(41, 43)\n",
      "(46, 52)\n",
      "(86, 71)\n",
      "(95, 81)\n",
      "(104, 73)\n",
      "(114, 67)\n",
      "(127, 63)\n",
      "(121, 78)\n",
      "(134, 64)\n",
      "(137, 69)\n",
      "(122, 80)\n",
      "(93, 77)\n",
      "(166, 58)\n",
      "(163, 62)\n",
      "(152, 63)\n",
      "(137, 84)\n",
      "(140, 85)\n",
      "(133, 75)\n",
      "(120, 80)\n",
      "(114, 77)\n",
      "(119, 74)\n",
      "(117, 81)\n",
      "(115, 110)\n",
      "(118, 112)\n",
      "(117, 114)\n",
      "(118, 113)\n",
      "(116, 113)\n",
      "(116, 115)\n",
      "(114, 113)\n",
      "(116, 112)\n",
      "(114, 112)\n",
      "(114, 114)\n",
      "(28, 23)\n",
      "(28, 23)\n",
      "(33, 23)\n",
      "(29, 23)\n",
      "(32, 23)\n",
      "(37, 23)\n",
      "(37, 24)\n",
      "(39, 25)\n",
      "(39, 25)\n",
      "(30, 25)\n",
      "(74, 67)\n",
      "(85, 67)\n",
      "(91, 67)\n",
      "(92, 66)\n",
      "(91, 67)\n",
      "(95, 67)\n",
      "(90, 67)\n",
      "(86, 68)\n",
      "(80, 66)\n",
      "(77, 67)\n",
      "(115, 69)\n",
      "(121, 53)\n",
      "(147, 47)\n",
      "(157, 48)\n",
      "(159, 49)\n",
      "(155, 50)\n",
      "(141, 52)\n",
      "(123, 65)\n",
      "(98, 69)\n",
      "(93, 79)\n",
      "(31, 39)\n",
      "(29, 27)\n",
      "(44, 37)\n",
      "(48, 44)\n",
      "(54, 48)\n",
      "(44, 48)\n",
      "(30, 31)\n",
      "(38, 40)\n",
      "(40, 42)\n",
      "(32, 38)\n",
      "(22, 31)\n",
      "(26, 30)\n",
      "(25, 28)\n",
      "(48, 44)\n",
      "(54, 48)\n",
      "(44, 48)\n",
      "(30, 31)\n",
      "(38, 40)\n",
      "(40, 42)\n",
      "(32, 38)\n",
      "(16, 42)\n",
      "(29, 27)\n",
      "(44, 37)\n",
      "(48, 44)\n",
      "(54, 48)\n",
      "(44, 48)\n",
      "(30, 31)\n",
      "(38, 40)\n",
      "(40, 42)\n",
      "(32, 38)\n",
      "(98, 84)\n",
      "(109, 87)\n",
      "(116, 87)\n",
      "(108, 90)\n",
      "(89, 91)\n",
      "(89, 79)\n",
      "(91, 73)\n",
      "(90, 100)\n",
      "(80, 104)\n",
      "(68, 103)\n",
      "(121, 67)\n",
      "(116, 83)\n",
      "(97, 82)\n",
      "(109, 88)\n",
      "(125, 90)\n",
      "(113, 94)\n",
      "(110, 93)\n",
      "(110, 89)\n",
      "(79, 39)\n",
      "(64, 45)\n",
      "(114, 112)\n",
      "(119, 111)\n",
      "(117, 111)\n",
      "(115, 113)\n",
      "(114, 113)\n",
      "(114, 113)\n",
      "(113, 112)\n",
      "(114, 114)\n",
      "(114, 113)\n",
      "(114, 112)\n",
      "(28, 25)\n",
      "(116, 83)\n",
      "(97, 82)\n",
      "(109, 88)\n",
      "(125, 90)\n",
      "(113, 94)\n",
      "(110, 93)\n",
      "(110, 89)\n",
      "(79, 39)\n",
      "(64, 45)\n",
      "(89, 84)\n",
      "(94, 77)\n",
      "(101, 68)\n",
      "(111, 71)\n",
      "(110, 69)\n",
      "(103, 66)\n",
      "(107, 77)\n",
      "(104, 74)\n",
      "(115, 75)\n",
      "(113, 72)\n",
      "(74, 69)\n",
      "(72, 70)\n",
      "(74, 68)\n",
      "(78, 68)\n",
      "(80, 69)\n",
      "(79, 69)\n",
      "(85, 69)\n",
      "(86, 71)\n",
      "(84, 69)\n",
      "(82, 69)\n",
      "(26, 35)\n",
      "(23, 74)\n",
      "(34, 82)\n",
      "(57, 94)\n",
      "(60, 89)\n",
      "(59, 86)\n",
      "(32, 69)\n",
      "(16, 55)\n",
      "(44, 38)\n",
      "(82, 69)\n",
      "(21, 88)\n",
      "(23, 74)\n",
      "(34, 82)\n",
      "(57, 94)\n",
      "(60, 89)\n",
      "(59, 86)\n",
      "(32, 69)\n",
      "(16, 55)\n",
      "(44, 38)\n",
      "(82, 69)\n",
      "(70, 105)\n",
      "(89, 115)\n",
      "(107, 102)\n",
      "(131, 79)\n",
      "(130, 68)\n",
      "(126, 73)\n",
      "(117, 87)\n",
      "(111, 106)\n",
      "(89, 98)\n",
      "(78, 94)\n",
      "(114, 113)\n",
      "(113, 112)\n",
      "(113, 113)\n",
      "(112, 112)\n",
      "(112, 113)\n",
      "(114, 113)\n",
      "(114, 114)\n",
      "(113, 113)\n",
      "(112, 113)\n",
      "(113, 114)\n",
      "(40, 21)\n",
      "(100, 78)\n",
      "(101, 71)\n",
      "(107, 89)\n",
      "(109, 94)\n",
      "(95, 104)\n",
      "(102, 105)\n",
      "(101, 99)\n",
      "(102, 94)\n",
      "(121, 84)\n",
      "(66, 55)\n",
      "(100, 78)\n",
      "(101, 71)\n",
      "(107, 89)\n",
      "(109, 94)\n",
      "(95, 104)\n",
      "(102, 105)\n",
      "(101, 99)\n",
      "(102, 94)\n",
      "(121, 84)\n",
      "(75, 68)\n",
      "(72, 75)\n",
      "(71, 72)\n",
      "(70, 71)\n",
      "(77, 71)\n",
      "(75, 68)\n",
      "(71, 71)\n",
      "(71, 69)\n",
      "(71, 69)\n",
      "(69, 71)\n",
      "(115, 65)\n",
      "(109, 54)\n",
      "(109, 48)\n",
      "(97, 58)\n",
      "(100, 62)\n",
      "(91, 72)\n",
      "(102, 73)\n",
      "(109, 53)\n",
      "(107, 50)\n",
      "(100, 52)\n",
      "(83, 86)\n",
      "(83, 69)\n",
      "(84, 82)\n",
      "(79, 106)\n",
      "(65, 115)\n",
      "(57, 107)\n",
      "(75, 97)\n",
      "(180, 70)\n",
      "(150, 82)\n",
      "(131, 66)\n",
      "(115, 113)\n",
      "(115, 113)\n",
      "(114, 113)\n",
      "(117, 113)\n",
      "(116, 114)\n",
      "(116, 115)\n",
      "(116, 116)\n",
      "(116, 115)\n",
      "(116, 115)\n",
      "(117, 114)\n",
      "(117, 79)\n",
      "(108, 65)\n",
      "(110, 70)\n",
      "(98, 77)\n",
      "(134, 81)\n",
      "(166, 79)\n",
      "(167, 77)\n",
      "(180, 70)\n",
      "(150, 82)\n",
      "(131, 66)\n",
      "(70, 70)\n",
      "(72, 70)\n",
      "(71, 71)\n",
      "(70, 72)\n",
      "(69, 72)\n",
      "(71, 72)\n",
      "(72, 71)\n",
      "(70, 71)\n",
      "(70, 70)\n",
      "(72, 71)\n",
      "(90, 61)\n",
      "(96, 62)\n",
      "(90, 63)\n",
      "(103, 69)\n",
      "(98, 70)\n",
      "(103, 75)\n",
      "(101, 75)\n",
      "(97, 71)\n",
      "(92, 67)\n",
      "(79, 59)\n",
      "(64, 33)\n",
      "(61, 44)\n",
      "(81, 60)\n",
      "(94, 60)\n",
      "(104, 69)\n",
      "(106, 54)\n",
      "(88, 47)\n",
      "(96, 64)\n",
      "(103, 66)\n",
      "(95, 62)\n",
      "(132, 69)\n",
      "(132, 74)\n",
      "(141, 69)\n",
      "(138, 72)\n",
      "(133, 62)\n",
      "(146, 58)\n",
      "(142, 62)\n",
      "(137, 68)\n",
      "(130, 73)\n",
      "(118, 93)\n",
      "(116, 117)\n",
      "(116, 115)\n",
      "(117, 114)\n",
      "(115, 114)\n",
      "(117, 115)\n",
      "(116, 115)\n",
      "(117, 116)\n",
      "(115, 115)\n",
      "(115, 114)\n",
      "(114, 114)\n",
      "(28, 27)\n",
      "(28, 26)\n",
      "(28, 27)\n",
      "(29, 27)\n",
      "(30, 26)\n",
      "(30, 26)\n",
      "(29, 25)\n",
      "(30, 25)\n",
      "(29, 25)\n",
      "(30, 26)\n",
      "(71, 70)\n",
      "(71, 71)\n",
      "(71, 70)\n",
      "(71, 70)\n",
      "(70, 72)\n",
      "(71, 70)\n",
      "(71, 72)\n",
      "(71, 70)\n",
      "(71, 70)\n",
      "(72, 74)\n",
      "(83, 62)\n",
      "(90, 67)\n",
      "(101, 70)\n",
      "(119, 76)\n",
      "(114, 75)\n",
      "(111, 72)\n",
      "(110, 68)\n",
      "(96, 73)\n",
      "(76, 77)\n",
      "(60, 67)\n",
      "(70, 84)\n",
      "(60, 84)\n",
      "(44, 90)\n",
      "(41, 94)\n",
      "(47, 84)\n",
      "(34, 81)\n",
      "(33, 58)\n",
      "(35, 58)\n",
      "(28, 39)\n",
      "(24, 37)\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "masks = [v['masks'] for v in video_extractor.data[3]['traces']]#this is a list of 10 masks\n",
    "\n",
    "\n",
    "#get make all the values == 1\n",
    "\n",
    "#padding\n",
    "shapes = []\n",
    "for cell_mask in masks: #there are 67 cells, each cell is a list of 10 cells\n",
    "    for cell_time in cell_mask:\n",
    "        cell_time = np.array(cell_time, dtype=np.int32)\n",
    "        cell_time = np.where(cell_time < 0, cell_time, 1)\n",
    "        print(cell_time.shape)\n",
    "        shapes.append(cell_time.shape)\n",
    "    \n",
    "print(np.max(shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "cell_time = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(np.max(cell_time))\n",
    "\n",
    "cell_time = np.where(cell_time < 1, cell_time, 1)\n",
    "\n",
    "\n",
    "print(np.max(cell_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([62.97384554, 26.70922169]),\n",
       " array([52.6810371 , 26.99937134]),\n",
       " array([45.47522825, 28.61106318]),\n",
       " array([85.75728009, 60.66285577]),\n",
       " array([32.28763764, 23.48785023]),\n",
       " array([30.01306352, 30.7486319 ]),\n",
       " array([19.88243374, 16.62626154]),\n",
       " array([11.69016281, 12.22928183]),\n",
       " array([57.40563179, 32.18842609]),\n",
       " array([55.30272335, 28.77540539])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage.measure import centroid\n",
    "\n",
    "test_patches = video_extractor.data[3]['traces'][0]['patches'] #this is a list of X paddings\n",
    "\n",
    "#get centroids\n",
    "\n",
    "centroids = ([centroid(t) for t in test_patches])\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in MIP 3\n",
      "Loading dicty_factin_pip3-03_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 90)}]\n",
      "frames 3: (90, 2048, 474)\n",
      "Extracting traces from 0:10\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting traces from 10:20\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 20:30\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 30:40\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 40:50\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 50:60\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 60:70\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting traces from 70:80\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting traces from 80:90\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "dict_keys(['patches', 'boxes', 'masks'])\n",
      "the number is cells is 67\n",
      "this is the keys of the first cell dict_keys(['boxes', 'patches', 'masks'])\n",
      "this is the amount of boxes in this cell 10\n",
      "there are  670  timesteps of  67  cells  this is what index they all start at \n",
      " [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670] \n",
      " and end \n",
      " [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670]\n"
     ]
    }
   ],
   "source": [
    "mip_video_files = [\n",
    "    ('mip', 3)\n",
    "]\n",
    "\n",
    "dataset = CellBoxMaskPatch(mip_video_files, 10) # file, S, T\n",
    "\n",
    "train, eval, test = random_split(dataset, [0.4, 0.2, 0.4])\n",
    "\n",
    "\n",
    "\n",
    "input_datasets = {}\n",
    "input_datasets[\"train\"] = train\n",
    "input_datasets[\"eval\"] = eval\n",
    "input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep = [[0] for t in range(T)]\n",
    "# timestep\n",
    "\n",
    "# batch = ((np.array([1 , 2, 3]), 1, 2) , 3), ((np.array([1 , 2, 3]), 1, 2) , 3)\n",
    "# current_boxes = torch.stack([torch.tensor(b[0][0], dtype=torch.long) for b in batch], dim=0)\n",
    "# current_boxes\n",
    "        #current     #next   #boxes                     #masks\n",
    "batch  = [[1, 2, 3], [[[1,2, 3, 4], [1, 2, 3,4]], [[1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1,2, 3]]]], [[1, 2, 3], [[[1,2, 3, 4], [1, 2, 3,4]], [[1, 2, 3], [1, 2, 3]], [[1, 2, 3], [1,2, 3]]]]\n",
    "\n",
    "\n",
    "\n",
    "next_boxes = torch.stack([torch.tensor(b[1][0], dtype=torch.long) for b in batch], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, mode_box, mode_mask, mode_patch, T):\n",
    "    #this should stick the batches together in way that makes sense in a torch stack manner\n",
    "    #we want to return dim [B, 1, d], [B, T, d] where d is a combination of 4 (for boxes) and the flattened image shape\n",
    "    #taking only taking one cell from sequence before and T is sequence after\n",
    "    #if mode_box is true, we want to return [B, 1, 4] and [B, T, 4]\n",
    "    #if mode_mask is true, we want to return [B, 1, (10,10).flatten] and [B, T, (10,10).flatten]\n",
    "    #input (cur_box, cur_mask, cur_patch), (T_box, T_mask, T_patch)\n",
    "  \n",
    "    current_boxes = torch.stack([torch.tensor(b[0][0], dtype=torch.long) for b in batch], dim=0)\n",
    "    current_masks = torch.stack([torch.tensor(b[0][1], dtype=torch.long) for b in batch], dim=0)\n",
    "    current_patches = torch.stack([torch.tensor(b[0][2], dtype=torch.long) for b in batch], dim=0)\n",
    "    selected_tensors = []\n",
    "\n",
    "\n",
    "    next_boxes = torch.stack([torch.tensor(b[1][0], dtype=torch.int) for b in batch], dim=0)\n",
    "    next_masks = torch.stack([torch.tensor(b[1][1], dtype=torch.float64) for b in batch], dim=0)\n",
    "    next_patches = torch.stack([torch.tensor(b[1][2], dtype=torch.float64) for b in batch], dim=0)\n",
    "    selected_next_tensors = []\n",
    "\n",
    "    if mode_box:\n",
    "        selected_tensors.append(current_boxes)\n",
    "        selected_next_tensors.append(next_boxes)\n",
    "    if mode_mask:\n",
    "        selected_tensors.append(current_masks)\n",
    "        selected_next_tensors.append(next_masks)\n",
    "    if mode_patch:\n",
    "        selected_tensors.append(current_patches)\n",
    "        selected_next_tensors.append(next_patches)\n",
    "\n",
    "\n",
    "    # Concatenate selected tensors along the last dimension\n",
    "    combined_tensor = torch.cat(selected_tensors, dim=-1)\n",
    "    combined_next_tensors = torch.cat(selected_next_tensors, dim=-1)\n",
    "\n",
    "    # Reshape to add the singleton dimension\n",
    "    combined_tensor = combined_tensor.unsqueeze(1)\n",
    "\n",
    "    # Verify the shape\n",
    "    # print(\"shape of input\", combined_tensor.shape, \"shape of labels of size\", T, \" \", combined_next_tensors.shape)\n",
    "\n",
    "\n",
    "    return combined_tensor, combined_next_tensors\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_box = True\n",
    "mode_mask = False\n",
    "mode_patch = False\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch, T)\n",
    ")\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['test'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch, T)\n",
    ")\n",
    "\n",
    "dataloaders['eval'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['eval'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch, T)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[[1, 2, 3, 5]],\n",
      "\n",
      "        [[1, 2, 3, 5]],\n",
      "\n",
      "        [[1, 2, 3, 5]],\n",
      "\n",
      "        [[1, 2, 3, 5]]]) next ones tensor([[[-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1]],\n",
      "\n",
      "        [[-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1]],\n",
      "\n",
      "        [[-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1]],\n",
      "\n",
      "        [[-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1],\n",
      "         [-1, -1, -1, -1]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders['train']:\n",
    "    # input, output = batch\n",
    "    # Process the batches as needed\n",
    "    # print(\"Input Shape:\", input.shape)\n",
    "    # print(\"Output Shape:\", output.shape)\n",
    "    print(\"Input:\", batch[0], \"next ones\", batch[1])\n",
    "    \n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an LSTM RNN\n",
    "\n",
    "#to create a pytorch LSTM \n",
    "#The first axis is the sequence itself\n",
    "# the second indexes instances in the mini-batch\n",
    "# third indexes elements of the input. \n",
    "\n",
    "# we would input T boxes\n",
    "batch_size = 4\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, T, hidden_dim, S, num_layers=2):\n",
    "        #(input is )\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "                           #input_size, hidden_size, num_layers\n",
    "        self.lstm = nn.LSTM(T, hidden_dim, num_layers=2, batch_first=True) #stacking 2 LSTMs\n",
    "\n",
    "        # The linear layer that maps from hidden state space to output space\n",
    "        self.fc = nn.Linear(hidden_dim, S)\n",
    "    def forward(self, input):\n",
    "\n",
    "\n",
    "        #h_0: tensor of shape (D∗num_layers,N,Hout​) containing the initial hidden\n",
    "        # state for each element in the input sequence. Defaults to zeros if (h_0, c_0) is not provided.\n",
    "        #D = 2 if bidirectional is True, 1 otherwise\n",
    "        #num layers is the number of layers\n",
    "        #N = size of input\n",
    "        #Hout is projection size\n",
    "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).to(input.device)\n",
    "        c0 = torch.zeros(self.num_layers, input.size(0), self.hidden_dim).to(input.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, _ = self.lstm(input, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        lstm_out, h_n = self.lstm(input, batch_size, -1) #output and final hidden state\n",
    "        output = self.hidden2out(lstm_out.view(len(input), -1))\n",
    "        #output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 3, 5), got [1, 3, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Forward pass with desired sequence length T\u001b[39;00m\n\u001b[1;32m     27\u001b[0m T \u001b[38;5;241m=\u001b[39m seq_length\n\u001b[0;32m---> 28\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/conda/envs/cvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/cvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     28\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# We need to detach as we are doing truncated backpropagation through time (BPTT)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# If we don't, we'll backprop all the way to the start even after going through another batch\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Decode the hidden state of the last time step\u001b[39;00m\n\u001b[1;32m     35\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m/opt/conda/envs/cvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/cvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/cvenv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:874\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    871\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/cvenv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:790\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    785\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    786\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    787\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    788\u001b[0m                        ):\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 790\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExpected hidden[0] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    793\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/cvenv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:259\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    257\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[0;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 3, 5), got [1, 3, 5]"
     ]
    }
   ],
   "source": [
    "#mip only had one zplane \n",
    "# Example input data\n",
    "#parameters for mask and patch\n",
    "data_dim =  1000 #TO DO: find the biggest box and set it to this\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "\n",
    "T = 10 #len predictions\n",
    "hidden_dims = 100\n",
    "output_dim = input_dim\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/ 1+ np.exp(-x)\n",
    "\n",
    "batch_size = 3\n",
    "seq_length = 4\n",
    "input_dim = 4\n",
    "hidden_dim = 5\n",
    "num_layers = 1\n",
    "#input is (batch, seq, feature)\n",
    "input_data = torch.randn(batch_size, input_dim, input_dim)\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = LSTM(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Forward pass with desired sequence length T\n",
    "T = seq_length\n",
    "output = lstm_model(input_data)\n",
    "\n",
    "print(\"Input shape:\", input_data.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dim = 0\n",
    "if mode_box == True:\n",
    "    input_dim +=  4 #(T, 4) for the bounding boxes\n",
    "if mode_mask == True:\n",
    "    input_dim += box_shape.flatten #flattened version of [H, W]\n",
    "if mode_patch == True:\n",
    "    input_dim += box_shape.flatten #flattened version of [H, W]\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 25\n",
    "T = 2\n",
    "hidden_dims = 100\n",
    "output_dim = input_dim\n",
    "batch_size = 1\n",
    "model = LSTM(input_dim, hidden_dims, output_dim)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your training loop + eval, etc...\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'dev']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:  #inputs shoudl be [B, S, D] #labes be a [B, T, D] batch, time, sequence shape\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                 # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, labels) #B, T, D\n",
    "\n",
    "                    #LOSS AND PRED EDIT STUFF HERE AS WE TALKED ABOUT\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'dev' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
