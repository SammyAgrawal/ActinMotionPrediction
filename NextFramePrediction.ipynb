{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca9078-71cf-41d4-aec7-2b775ab3188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aicspylibczi import CziFile\n",
    "import czifile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import cv2\n",
    "import os\n",
    "import imageio\n",
    "import ffmpeg\n",
    "import time\n",
    "import pandas as pd\n",
    "# from cellpose import io, models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "cudnn.benchmark = True\n",
    "from VideoLoaders import *\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a425c-002e-4cdc-bafa-8f7f024123fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataMIP:\n",
    "    def __init__(self, files):\n",
    "        self.data = {\n",
    "        }\n",
    "        \n",
    "        for category, num in files:\n",
    "            print(f\"Loading in MIP {num}\")\n",
    "            print(category)\n",
    "            # assert category == 'mip', \"Can't load non Mip file\"\n",
    "            file = {}\n",
    "            file['video'] = get_file(category, num)\n",
    "            \n",
    "            frames, shp = file['video'].read_image(C=0)\n",
    "            frames = scale_img(frames.squeeze())\n",
    "            file['frames'] = frames\n",
    "            print(f\"frames {num}: {frames.shape}\")\n",
    "            file['masks'] = binarize_video(frames)           \n",
    "    \n",
    "            self.data[num] = file    \n",
    "    def extract_all_traces(self, file_num, sequence_length, hist_length=2):\n",
    "        # hist length is how many frames of history\n",
    "        frames, masks = self.data[file_num]['frames'], self.data[file_num]['masks']\n",
    "        N = len(frames)\n",
    "        s = 0\n",
    "        all_traces = []\n",
    "        all_videos = []\n",
    "        for i in range(N // sequence_length):\n",
    "            print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "            data, videos = extract_traces(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "            s += sequence_length\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos\n",
    "\n",
    "\n",
    "        \n",
    "        if(N % sequence_length > 0):\n",
    "            data, videos = extract_traces(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos \n",
    "            \n",
    "                \n",
    "        self.data[file_num]['traces'] = all_traces\n",
    "        self.data[file_num]['trace_videos'] = all_videos\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed79b05-c94d-476e-b64d-a577539b67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import centroid\n",
    "import skimage.measure as skm\n",
    "\n",
    "max_padding =  300\n",
    "\n",
    "box_shape = (180, 180) #TO DO: find the biggest box and set it to this\n",
    "X = 10\n",
    "\n",
    "class CellBoxMaskPatch(torch.utils.data.Dataset):\n",
    "    #input will be a Directory name, function is TO DO\n",
    "    def __init__(\n",
    "        self,\n",
    "        files, \n",
    "        X=X):\n",
    "        \n",
    "        self.mips_extractor = VideoDataMIP(files)\n",
    "        # self.proc_extractor = VideoDataProcessed(files)\n",
    "\n",
    "        for i in files:\n",
    "            self.mips_extractor.extract_all_traces(i[1], X)\n",
    "        \n",
    "        \n",
    "        self.cell_dict = []\n",
    "\n",
    "        for key in self.mips_extractor.data:\n",
    "            entry = self.mips_extractor.data[key][\"traces\"]\n",
    "            for cell in entry:\n",
    "                patches = [np.array(p) for p in cell[\"patches\"]]\n",
    "                boxes = [np.array(b) for b in cell['boxes']]\n",
    "                masks = [np.array(m) for m in cell['masks']]\n",
    "                \n",
    "                self.cell_dict.append((boxes, masks, patches)) #cell dict is a list of 3 types by sequence\n",
    "\n",
    "        self.num_cells = len(self.cell_dict) #this is a list of how many sequences we have\n",
    "              \n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "        \n",
    "\n",
    "    def get_centroids(self, boxes, masks):\n",
    "        N = len(masks)\n",
    "        res = []\n",
    "        centroids = [skm.centroid(binary.astype(np.uint8)) for binary in masks]\n",
    "        for i in range(N):\n",
    "            c = centroids[i]\n",
    "            ymin, xmin = boxes[i][:2]\n",
    "            res.append([xmin+c[0], ymin+c[1]])\n",
    "        return(np.array(res) - res[0]) \n",
    "   \n",
    "    def pad_arrays(self, array, pad_amt=max_padding):\n",
    "    \n",
    "        pad_width = ((0, pad_amt - array.shape[0]), (0, pad_amt - array.shape[1]))\n",
    "\n",
    "        padded_array = np.pad(array, pad_width, mode='constant')\n",
    "        return padded_array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_sequences = self.cell_dict[idx]  #this is the first sequence of 10 cells\n",
    "        boxes = cell_sequences[0]\n",
    "        masks = cell_sequences[1]\n",
    "        patches = cell_sequences[2]\n",
    "\n",
    "\n",
    "        for cell_mask_num in np.arange(len(masks)): #should be sequence length (10) masks\n",
    "                # print(patches[cell_mask_num].shape)      \n",
    "                # plt.imshow(patches[cell_mask_num], cmap='viridis')\n",
    "                # plt.colorbar()\n",
    "                # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                cell_time = np.array(masks[cell_mask_num], dtype=np.int32)\n",
    "                cell_time = np.where(cell_time >= 0, cell_time, 1)\n",
    "                cell_time = self.pad_arrays(cell_time)\n",
    "                masks[cell_mask_num] = cell_time\n",
    "                cell_time_patch = np.array(patches[cell_mask_num])\n",
    "                \n",
    "                # print(cell_time_patch.shape)      \n",
    "                # plt.imshow(cell_time_patch, cmap='viridis')\n",
    "                # plt.colorbar()\n",
    "                # plt.show()\n",
    "\n",
    "                cell_time_patch = self.pad_arrays(cell_time_patch)\n",
    "\n",
    "                patches[cell_mask_num] = cell_time_patch\n",
    "\n",
    "                # print(patches[cell_mask_num].shape)      \n",
    "                # plt.imshow(patches[cell_mask_num], cmap='viridis')\n",
    "                # plt.colorbar()\n",
    "                # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        centroids = self.get_centroids(boxes, masks)\n",
    "    \n",
    "\n",
    "        return centroids, masks, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e324da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "mip_video_files = [\n",
    "    ('mip', 3),\n",
    "    # ('mip', 6),\n",
    "    # ('mip', 9)\n",
    "]\n",
    "\n",
    "dataset = CellBoxMaskPatch(mip_video_files, X) # file, S, T\n",
    "\n",
    "train, eval, test = random_split(dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "input_datasets = {}\n",
    "input_datasets[\"train\"] = train\n",
    "input_datasets[\"eval\"] = eval\n",
    "input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d04ff-2a4d-4226-9838-026a10d37ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, mode_box, mode_mask, mode_patch):\n",
    "    label = 0\n",
    "    current_centroids = [b[0] for b in batch]\n",
    "    current_masks = [b[1] for b in batch]\n",
    "    current_patches = [b[2] for b in batch]\n",
    "\n",
    "\n",
    "    # plt.imshow(current_patches[0][0], cmap='viridis')\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "\n",
    "    current_centroids = torch.tensor(np.stack(current_centroids), dtype=torch.float32)\n",
    "    current_masks = torch.tensor(np.stack(current_masks), dtype=torch.float32)\n",
    "    current_patches = torch.tensor(np.stack(current_patches), dtype=torch.float32)\n",
    "\n",
    "    # plt.imshow(current_patches[0][0], cmap='viridis')\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "\n",
    "    current_patches = current_patches.reshape([len(batch), 1, 10, max_padding, max_padding])\n",
    "    current_masks = current_masks.reshape([len(batch), 1, 10, max_padding, max_padding])\n",
    "\n",
    "\n",
    "    return current_masks, current_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f83b05-0e73-4f12-8ca1-a207a201b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box is actually a box surrounding the cell\n",
    "# mask is the values of the cell\n",
    "# patch is fluorescence\n",
    "mode_box = True\n",
    "mode_mask = True\n",
    "mode_patch = False\n",
    "\n",
    "input_size = 0\n",
    "if mode_box:\n",
    "    input_size+=2\n",
    "if mode_mask:\n",
    "    input_size+=max_padding*max_padding\n",
    "if mode_patch:\n",
    "    input_size+=max_padding*max_padding\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['test'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['eval'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['eval'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f225f6-9e7f-4b15-bd8b-34d63d3b8b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloaders['eval']:\n",
    "    print(\"Masks:\", batch[0].shape, \"Patches\", batch[1].shape)\n",
    "    break\n",
    "    \n",
    "#was Input: torch.Size([3, 10, 90000]) Centroids torch.Size([3, 10, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ab035",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568655b8-8ed2-4097-829f-0423b5afdd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv_3d_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_3d_CNN, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(1, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3d(x.float())\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Comp_3d_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Comp_3d_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv3d(64, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.float())\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_3d_CNNlSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_3d_CNNlSTM, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=64 * 75 * 75, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.decoder = nn.Linear(128, 300 * 300)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input for CNN\n",
    "        B, C, T, H, W = x.size()\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        \n",
    "        # Apply CNN\n",
    "        cnn_out = self.cnn(x)\n",
    "        cnn_out = cnn_out.view(B, T, -1)\n",
    "        \n",
    "        # Apply LSTM\n",
    "        lstm_out, _ = self.lstm(cnn_out)\n",
    "        lstm_out = lstm_out[:, -1, :]  # take the output from the last time step\n",
    "        \n",
    "        # Decode to next frame\n",
    "        next_frame = self.decoder(lstm_out)\n",
    "        next_frame = next_frame.view(B, 1, 1, 300, 300)\n",
    "        \n",
    "        return next_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb82b3-20cd-44eb-9f9f-43a55697cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 2000\n",
    "num_layers = 2\n",
    "epochs = 100\n",
    "sequence_length = 10 #how many frames we process per input\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# model = LSTM(input_size, hidden_size, num_layers)\n",
    "model = final\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# dummy_input_data = torch.randn(batch_size, 10, input_size)\n",
    "\n",
    "input_type = 'patch'\n",
    "output_type = 'mask'\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in dataloaders['train']:\n",
    "        optimizer.zero_grad()\n",
    "        masks, patches = batch[0], batch[1]\n",
    "\n",
    "        if input_type == \"mask\":\n",
    "            inputs = masks\n",
    "        else:\n",
    "            inputs = patches\n",
    "\n",
    "        if output_type == \"masks\":\n",
    "            outputs = masks\n",
    "        else:\n",
    "            outputs = patches\n",
    "     \n",
    "        inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "        pred = model(inputs[:, :sequence_length-1, :])\n",
    "      \n",
    "        loss = criterion(pred.squeeze(), outputs[:,-1,:].float())*1000\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"training loss: {total_loss / len(dataloaders['train'])}\")\n",
    "    return model, total_loss / len(dataloaders['train'])\n",
    "\n",
    "def eval():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloaders['eval']:\n",
    "            masks, patches = batch[0], batch[1]\n",
    "            if input_type == \"mask\":\n",
    "                inputs = masks\n",
    "            else:\n",
    "                inputs = patches\n",
    "\n",
    "            if output_type == \"masks\":\n",
    "                outputs = masks\n",
    "            else:\n",
    "                outputs = patches\n",
    "\n",
    "\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            pred = model(inputs[:, :sequence_length-1, :]).squeeze()\n",
    " \n",
    "            loss = criterion(pred, outputs[:,-1,:])*1000\n",
    "            total_loss += loss.item()\n",
    "    print(f\"validation loss: {total_loss / len(dataloaders['eval'])}\")\n",
    "    return total_loss / len(dataloaders['eval'])\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        mod, loss_t = train()\n",
    "        train_loss.append(loss_t)\n",
    "        curr_acc = eval()\n",
    "        val_loss.append(curr_acc)\n",
    "        if curr_acc > best_acc:\n",
    "            best_acc = curr_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    y1 = train_loss\n",
    "    y2 = val_loss\n",
    "\n",
    "    plt.plot(range(len(y1)), y1, color='blue', label='Train Loss')\n",
    "    plt.plot(range(len(y2)), y2, color='red', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(best_model_wts)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d560a2-0af0-4374-bc65-739e0d8713e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146ac4d-71f3-4f35-8000-175fa16b7fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_layer_cnn_model = final\n",
    "\n",
    "for batch in dataloaders['train']:\n",
    "    masks, patches = batch[0], batch[1]\n",
    "\n",
    "    if input_type == \"mask\":\n",
    "            inputs = masks\n",
    "    else:\n",
    "            inputs = patches\n",
    "\n",
    "    if output_type == \"masks\":\n",
    "            outputs = masks\n",
    "    else:\n",
    "            outputs = patches\n",
    "\n",
    "\n",
    "    inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "    print(outputs.shape)\n",
    "    pred = model(inputs[:, :sequence_length-1, :])\n",
    "    print(pred.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(10):\n",
    "        plt.imshow(outputs[0][0][i].detach().numpy(), cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "       #prediction\n",
    "    plt.imshow(pred[0][0][0].detach().numpy(), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    # plt.imshow(outputs[:,-1,:][0][0].detach().numpy(), cmap='viridis')\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # plt.imshow(outputs[:,-1,:][0][0].detach().numpy(), cmap='viridis')\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "    break\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752460f6-f7a6-4b21-978d-d494889ad2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
