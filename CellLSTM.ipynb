{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f8e87bacbd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aicspylibczi import CziFile\n",
    "import czifile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import os\n",
    "import imageio\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VideoDataMIP:\n",
    "    def __init__(self, files):\n",
    "        self.data = {\n",
    "        }\n",
    "        \n",
    "        for category, num in files:\n",
    "            print(f\"Loading in MIP {num}\")\n",
    "            assert category == 'mip', \"Can't load non Mip file\"\n",
    "            file = {}\n",
    "            file['video'] = get_file(category, num)\n",
    "            \n",
    "            frames, shp = file['video'].read_image(C=0)\n",
    "            frames = scale_img(frames.squeeze())\n",
    "            file['frames'] = frames\n",
    "            print(f\"frames {num}: {frames.shape}\")\n",
    "            file['masks'] = binarize_video(frames)           \n",
    "    \n",
    "            self.data[num] = file    \n",
    "    def extract_all_traces(self, file_num, sequence_length, hist_length=2):\n",
    "        # hist length is how many frames of history\n",
    "        frames, masks = self.data[file_num]['frames'], self.data[file_num]['masks']\n",
    "        N = len(frames)\n",
    "        s = 0\n",
    "        all_traces = []\n",
    "        all_videos = []\n",
    "        for i in range(N // sequence_length):\n",
    "            print(f\"Extracting traces from {s}:{s+sequence_length}\")\n",
    "            data, videos = extract_traces(frames[s:s+sequence_length], masks[s:s+sequence_length], hist=hist_length)\n",
    "            s += sequence_length\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos\n",
    "        \n",
    "        if(N % sequence_length > 0):\n",
    "            data, videos = extract_traces(frames[-1*sequence_length:], masks[-1*sequence_length:], hist=hist_length)\n",
    "            all_traces = all_traces + data\n",
    "            all_videos = all_videos + videos        \n",
    "        self.data[file_num]['traces'] = all_traces\n",
    "        self.data[file_num]['trace_videos'] = all_videos\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import centroid\n",
    "import skimage.measure as skm\n",
    "\n",
    "\n",
    "\n",
    "box_shape = (180, 180) #TO DO: find the biggest box and set it to this\n",
    "X = 10\n",
    "\n",
    "class CellBoxMaskPatch(torch.utils.data.Dataset):\n",
    "    #input will be a Directory name, function is TO DO\n",
    "    def __init__(\n",
    "        self,\n",
    "        files, \n",
    "        X=X):\n",
    "        \n",
    "        self.video_extractor = VideoDataMIP(files)\n",
    "\n",
    "        for i in files:\n",
    "            self.video_extractor.extract_all_traces(i[1], X)\n",
    "        \n",
    "        \n",
    "        self.cell_dict = []\n",
    "\n",
    "        for key in self.video_extractor.data:\n",
    "            entry = self.video_extractor.data[key][\"traces\"]\n",
    "            for cell in entry:\n",
    "                patches = [np.array(p) for p in cell[\"patches\"]]\n",
    "                boxes = [np.array(b) for b in cell['boxes']]\n",
    "                masks = [np.array(m) for m in cell['masks']]\n",
    "                \n",
    "                self.cell_dict.append((boxes, masks, patches)) #cell dict is a list of 3 types by sequence\n",
    "\n",
    "        self.num_cells = len(self.cell_dict) #this is a list of how many sequences we have\n",
    "              \n",
    "    def __len__(self):\n",
    "        return self.num_cells\n",
    "        \n",
    "\n",
    "    def get_centroids(self, boxes, masks):\n",
    "        N = len(masks)\n",
    "        res = []\n",
    "        centroids = [skm.centroid(binary.astype(np.uint8)) for binary in masks]\n",
    "        for i in range(N):\n",
    "            c = centroids[i]\n",
    "            ymin, xmin = boxes[i][:2]\n",
    "            res.append([xmin+c[0], ymin+c[1]])\n",
    "        return(np.array(res) - res[0]) \n",
    "   \n",
    "    def pad_arrays(self, array, pad_amt=200):\n",
    "    \n",
    "        pad_width = ((0, pad_amt - array.shape[0]), (0, pad_amt - array.shape[1]))\n",
    "\n",
    "        padded_array = np.pad(array, pad_width, mode='constant')\n",
    "        return padded_array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_sequences = self.cell_dict[idx]  #this is the first sequence of 10 cells\n",
    "        boxes = cell_sequences[0]\n",
    "        masks = cell_sequences[1]\n",
    "        patches = cell_sequences[2]\n",
    "\n",
    "\n",
    "        for cell_mask_num in np.arange(len(masks)): #should be sequence length (10) masks\n",
    "                \n",
    "                cell_time = np.array(masks[cell_mask_num], dtype=np.int32)\n",
    "                cell_time = np.where(cell_time >= 0, cell_time, 1)\n",
    "                cell_time = self.pad_arrays(cell_time)\n",
    "                masks[cell_mask_num] = cell_time\n",
    "                cell_time_patch = np.array(patches[cell_mask_num], dtype=np.int32)\n",
    "\n",
    "                cell_time_patch = self.pad_arrays(cell_time_patch)\n",
    "\n",
    "                patches[cell_mask_num] = cell_time_patch\n",
    "\n",
    "\n",
    "        centroids = self.get_centroids(boxes, masks)\n",
    "    \n",
    "\n",
    "        return centroids, masks, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in MIP 3\n",
      "Loading dicty_factin_pip3-03_MIP.czi with dims [{'X': (0, 474), 'Y': (0, 2048), 'C': (0, 2), 'T': (0, 90)}]\n",
      "frames 3: (90, 2048, 474)\n",
      "Extracting traces from 0:10\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting traces from 10:20\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 20:30\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 30:40\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting traces from 40:50\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n",
      "Extracting cell  8\n",
      "Extracting traces from 50:60\n",
      "Extracting cell  0\n",
      "Extracting cell  1\n",
      "Extracting cell  2\n",
      "Extracting cell  3\n",
      "Extracting cell  4\n",
      "Extracting cell  5\n",
      "Extracting cell  6\n",
      "Extracting cell  7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[1;32m      3\u001b[0m mip_video_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CellBoxMaskPatch(mip_video_files, \u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# file, S, T\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train, \u001b[38;5;28meval\u001b[39m, test \u001b[38;5;241m=\u001b[39m random_split(dataset, [\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.4\u001b[39m])\n\u001b[1;32m     11\u001b[0m input_datasets \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mCellBoxMaskPatch.__init__\u001b[0;34m(self, files, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_extractor \u001b[38;5;241m=\u001b[39m VideoDataMIP(files)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_extractor\u001b[38;5;241m.\u001b[39mextract_all_traces(i[\u001b[38;5;241m1\u001b[39m], X)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_dict \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_extractor\u001b[38;5;241m.\u001b[39mdata:\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mVideoDataMIP.extract_all_traces\u001b[0;34m(self, file_num, sequence_length, hist_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m sequence_length):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting traces from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m+\u001b[39msequence_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     data, videos \u001b[38;5;241m=\u001b[39m extract_traces(frames[s:s\u001b[38;5;241m+\u001b[39msequence_length], masks[s:s\u001b[38;5;241m+\u001b[39msequence_length], hist\u001b[38;5;241m=\u001b[39mhist_length)\n\u001b[1;32m     29\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sequence_length\n\u001b[1;32m     30\u001b[0m     all_traces \u001b[38;5;241m=\u001b[39m all_traces \u001b[38;5;241m+\u001b[39m data\n",
      "File \u001b[0;32m~/CVProject/utils.py:184\u001b[0m, in \u001b[0;36mextract_traces\u001b[0;34m(frames, masks, hist, load_verifying_videos)\u001b[0m\n\u001b[1;32m    182\u001b[0m     vid_data\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(load_verifying_videos):\n\u001b[0;32m--> 184\u001b[0m         videos_for_checking\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(visualize_cell_tracker(frames, data)))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(vid_data, videos_for_checking)\n",
      "File \u001b[0;32m~/CVProject/utils.py:150\u001b[0m, in \u001b[0;36mvisualize_cell_tracker\u001b[0;34m(frames, data, thickness, val)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames)):\n\u001b[1;32m    149\u001b[0m     box \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m][frame_idx]\n\u001b[0;32m--> 150\u001b[0m     img \u001b[38;5;241m=\u001b[39m draw(frames[frame_idx]\u001b[38;5;241m.\u001b[39mcopy(), box, thickness, val)\n\u001b[1;32m    151\u001b[0m     video_frames\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(video_frames)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "mip_video_files = [\n",
    "    ('mip', 3)\n",
    "]\n",
    "\n",
    "dataset = CellBoxMaskPatch(mip_video_files, 10) # file, S, T\n",
    "\n",
    "train, eval, test = random_split(dataset, [0.4, 0.2, 0.4])\n",
    "\n",
    "input_datasets = {}\n",
    "input_datasets[\"train\"] = train\n",
    "input_datasets[\"eval\"] = eval\n",
    "input_datasets[\"test\"] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, mode_box, mode_mask, mode_patch):\n",
    "    \n",
    "\n",
    "    # centroids = torch.stack([torch.tensor(b[0], dtype=torch.int) for b in batch], dim=0)\n",
    "    # print(centroids.shape)\n",
    "    # current_masks = torch.stack([torch.tensor(b[1], dtype=torch.long) for b in batch], dim=0)\n",
    "    # current_masks = current_masks.reshape([4, 10, 40000])\n",
    "\n",
    "    current_centroids = [b[0] for b in batch]\n",
    "    current_masks = [b[1] for b in batch]\n",
    "    current_patches = [b[2] for b in batch]\n",
    "\n",
    "    current_centroids = torch.tensor(np.stack(current_centroids), dtype=torch.float32)\n",
    "    current_masks = torch.tensor(np.stack(current_masks), dtype=torch.long)\n",
    "    current_patches = torch.tensor(np.stack(current_patches), dtype=torch.long)\n",
    "\n",
    "    \n",
    "    current_patches = current_patches.reshape([4, 10, 40000])\n",
    "    current_masks = current_masks.reshape([4, 10, 40000])\n",
    "\n",
    "    print(current_masks.shape)\n",
    "    print(current_patches.shape)\n",
    "\n",
    "    selected_tensors = []\n",
    "    if mode_box:\n",
    "        selected_tensors.append(current_centroids)\n",
    "    if mode_mask:\n",
    "        selected_tensors.append(current_masks)\n",
    "    if mode_patch:\n",
    "        selected_tensors.append(current_patches)\n",
    "\n",
    "    # Concatenate selected tensors along the last dimension\n",
    "    combined_tensor = torch.cat(selected_tensors, dim=-1)\n",
    "\n",
    "    # Reshape to add the singleton dimension\n",
    "    combined_tensor = combined_tensor #.unsqueeze(1)\n",
    "\n",
    "    return combined_tensor, current_centroids\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mode_box = True\n",
    "mode_mask = True\n",
    "mode_patch = False\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['train'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['test'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")\n",
    "\n",
    "dataloaders['eval'] = torch.utils.data.DataLoader(\n",
    "    input_datasets['eval'],\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda batch: collate_fn(batch, mode_box, mode_mask, mode_patch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 40000])\n",
      "torch.Size([4, 10, 40000])\n",
      "Input: torch.Size([4, 10, 40002]) Centroids torch.Size([4, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders['train']:\n",
    "\n",
    "    print(\"Input:\", batch[0].shape, \"Centroids\", batch[1].shape)\n",
    "    \n",
    "    break  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
